%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%% RÉSUMÉ ET INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frontmatter
\chapter*{Résumé}
\addcontentsline{toc}{chapter}{Résumé}
Le présent mémoire présente certains aspects d'une chaîne de traitement consacrée à un corpus de catalogues de vente de manuscrits datant du \scl{XIX} au début du \scl{XX} siècle. Ces catalogues forment un corpus de données semi-structurées, puisqu'ils sont organisés sous la forme d'une liste d'items de manuscrits qui sont toujours décrits de façon semblable. Grâce à cette nature semi-structurée des documents, il est possible de développer une chaîne de traitement entièrement basée sur la détection de motifs, c'est-à-dire sur l'identification d'éléments récurrents d'une entrée à l'autre. Le fil conducteur de ce mémoire est donc une analyse de la manière dont la nature semi-structurée du corpus peut être mobilisée pour analyser, manipuler et diffuser des données textuelles. Le présent texte s'intéresse notamment à la manière dont les documents sont encodés et aux aspects des catalogues imprimés qui sont sélectionnés pour produire un encodage manipulable automatiquement. Ensuite, ce mémoire présente une chaîne de traitement pour aligner les noms d'auteur.ice de manuscrits mentionné.e.s dans les catalogues avec la base de connaissance en ligne \wkd{}. Cette chaîne de traitement s'appuie sur des algorithmes de détection et de transformation de motifs dans le texte, ainsi que sur un algorithme faisant des recherches sur l'API de \wkd{}. Elle permet de constituer à l'aide de \texttt{SPARQL} une base de connaissances. Celle-ci servira notamment à mener une étude des facteurs biographiques influençant le prix des manuscrits. Enfin est présenté le fonctionnement de \textit{KatAPI}, une API pour le partage automatisé de données produites par le projet. En plus de présenter les standards sur lesquels s'appuie cette API sont décrits les principes architecturaux et le fonctionnement interne de l'application.

\vfill

\noindent\textbf{Mots clés}: catalogues de vente, \mssktb, traitement automatisé du langage, détection de motifs, Web sémantique, \textit{Linked Open Data}, API, FAIR, REST

\chapter*{Remerciements}
\addcontentsline{toc}{chapter}{Remerciements}
Je tiens avant tout à remercier chaleureusement Mme Lucence Ing d'avoir accepté de diriger ce mémoire. Je la remercie pour ses orientations, ses conseils et ses relectures sans lesquelles ce mémoire aurait pris une forme très différente.

Je me dois ensuite de remercier mes tuteur.ice.s de stage: Léa Saint-Raymond et Simon Gabay. Je les remercie avant tout d'avoir bien voulu accepter de m'accueillir au sein du projet \mssktb{}. Je remercie Simon Gabay pour la patience dont il a sû faire preuve en m'expliquant à de multiples reprises les subtilités de l'édition \xmltei{} des catalogues de vente de manuscrits; de même, je remercie Léa Saint-Raymond pour ses nombreux conseils et ses orientations sur les questions statistiques et économiques, et bien sûr pour les orientations scientifiques qu'elle m'a fourni tout au long de ce stage. Leur sympathie et leur disponibilité a fait de ce stage une expérience aussi agréable qu'enrichissante. Il me faut également remercier toutes les personnes à l'École normale supérieure (ENS) qui m'ont accueilli au sein de cet établissement.

Je tiens également à remercier ici mon camarade Virgile Reigner pour ses orientations bibliographiques sur la reconnaissance d'entités nommées, qui ont eu une influence significative sur la seconde partie de ce mémoire et qui m'ont permis de prendre une distance critique sur les questions techniques discutées dans cette partie.

Enfin, il me faut pour finir remercier Roberto, mascotte de l'Observatoire des humanités numériques à l'ENS, pour avoir été une constante source d'inspiration au cours de ce stage.

\mainmatter
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\markboth{\textsc{INTRODUCTION}}{\textsc{INTRODUCTION}}
Depuis les années 1950 jusqu'à leur décès en 2007 et 2015, les photographes allemand.e.s Bernd et Hilla Becher se sont consacré.e.s à un projet simple: photographier le patrimoine industriel européen et américain. Le travail des artistes est frappant par son homogénéité, fruit d'un processus de travail particulièrement rigoureux. Les objets (châteaux d'eau, silos à grains, réservoirs à gaz) sont toujours photographiés à une distance égale, de sorte à couvrir l'intégralité du premier plan d'une photographie, de face, sur un fond uniforme, en noir et blanc. Les différentes photographes, de format réduit, sont exposées ensemble dans une grille comprenant neuf à douze images. Les photographies n'ont pas une mission documentaire ou informative: elles fonctionnent sur un plan purement formel et esthétique. L'objectif assumé est de créer une typologie de formes industrielles; la présentation en grille encourage les spectateur.ice.s à s'éloigner de l'objet isolé pour avoir un regard comparatif, percevoir les différences de formes entre les différents objets photographiés. L'objectif de leur travail est de créer \enquote{une grammaire permettant de comparer et de comprendre les différentes structures}\footnote{
	\enquote{\textit{We want to offer the audience a point of view, or rather a grammar, to understand and compare the different structures}} -- Bernd et Hilla Becher dans \cite{stimson_photographic_2004}
}. Les termes de grammaire et de typologie sont intéressants: les photographies ne visent pas à représenter une chose unique et originale dans une approche humaniste (ce qui est, par exemple, une des fonctions du portrait). La photographie permet au contraire de prendre de la hauteur, d'encadrer et de rassembler des objets dans une collection afin de permettre une analyse structurelle, un regard qui se concentre non pas sur l'objet, mais sur les récurrences formelles. La photographie, ici, par sa capacité à décontextualiser et à défonctionnaliser un objet pour le réintégrer dans une grille, permet un changement de rapport à ce qui est photographié. Le ou la spectateur.ice ne voit plus un objet fonctionnel, mais une structure qui se décline dans divers objets du réel.

Cette approche structurelle du réel, qui consiste à ne pas s'intéresser (seulement) à l'objet en lui-même, mais à le percevoir comme occurrence d'une classe, s'accompagne d'un processus inverse: la modélisation. Celle-ci correspond à l'établissement d'un modèle abstrait, qui sera ensuite appliqué pour organiser et représenter différents objets du réel. C'est ce processus de modélisation qui est à la base du projet \mssktb{}. 

Le projet débute avec \ktb{}, une une extension de \textit{e-Ditiones}. Ce dernier projet de recherche, créé en 2018 et coordonné par Simon Gabat, est consacré à l'établissement d'un catalogue de manuscrits français du \scl{XIX} à nos jours. Dans l'écosystème \textit{e-Ditiones}, le projet \ktb{}, lui aussi dirigé par Simon Gabay, se consacre à l'étude des catalogues de vente de manuscrits; ce projet est augmenté en 2021 par \mss{}, co-dirigé par Léa Saint-Raymond et Simon Gabay. C'est à l'intersection de \mss{} et de \ktb{} que s'est déroulé mon stage. Au sein de ces projets, j'ai été chargé, d'avril à septembre 2022, de compléter la chaîne de traitement des catalogues numérisés et encodés en \xmltei{}. Cette chaîne de traitement permet la fouille automatique des catalogues afin d'en extraire des données normalisées, ainsi qu'à la diffusion de ces données sur le \href{https://katabase.huma-num.fr/}{site internet du projet \ktb{}}. Mon activité s'est principalement concentrée sur le développement de nouvelles fonctionnalités pour la chaîne de traitement. D'abord, des outils ont été développés afin d'automatiser l'alignement d'entrées de manuscrits avec la base de données en ligne \wkd{}, afin de permettre l'enrichissement des catalogues en vue d'une étude économétrique de ceux-ci. Ensuite, le site Web du projet a été augmenté de nouvelles fonctionnalités, et notamment d'une \gls{api}, permettant la diffusion automatisée des données du projet grâce à l'interaction de machine à machine.

Dans le titre de ce mémoire, je définis les catalogues de vente comme formant un corpus textuel semi-structuré. Ce dernier terme mérite d'être défini. Je considère ces documents comme semi-structurés, car, indépendamment de tout encodage, ils présentent une structure particulièrement régulière, qui se retrouve d'un catalogue à un autre. Malgré cette structure, les catalogues restent des documents textuels, et contiennent donc du texte libre et non structuré. Un document semi-structuré est donc un document utilisant du texte libre et des formulations conventionnelles à l'intérieur d'une structure régulière et définie formellement. Dans le cas d'un corpus imprimé, ce sont des choix typographiques qui mettent en avant la structure du document, notamment par l'usage de sauts de lignes, de capitales, de changements de graisse de la fonte ainsi que d'italiques. Le propre de nombreux documents semi-structurés est qu'ils sont organisés en entrées qui contiennent toutes le même type d'informations: c'est le cas par exemple des catalogues, des inventaires et des dictionnaires. Dire qu'un document est semi-structuré implique, nécessairement, une forme de modélisation de celui-ci: dans le document est identifié une structure abstraite qui est distincte de son contenu textuel. L'encodage du document dans un format numérique consiste alors à expliciter le processus de modélisation, à construire de manière formelle, en utilisant un format structuré, un modèle formel à l'intérieur duquel intégrer le texte des catalogues. De par l'usage d'expressions conventionnelles et de structures répétitives, une structure peut être identifiée dans un document à une échelle inférieure à ce qu'il est possible d'encoder numériquement. À l'intérieur d'un même élément, des motifs se répètent, des structures peuvent être identifiées. Expliciter celles-ci peut servir de base à une chaîne de traitement automatisé et de fouille du texte, comme cela a été fait au sein du projet \mssktb{}.

Les termes de modélisation, enrichissement sémantique et diffusion forment trois manières distinctes d'aborder un corpus, et c'est en suivant ces trois approches que le corpus de catalogues de vente de manuscrits est présenté dans le présent mémoire. La modélisation implique de réfléchir à la structure et à l'organisation du corpus. Dans le contexte numérique, modéliser un texte revient à l'inclure dans des éléments hiérarchisés ayant une sémantique précise. La modélisation numérique du texte amène à produire un encodage qui fonctionne comme un \enquote{méta-texte}, comme une interprétation et une analyse du texte qui est encodée à l'intérieur de celui-ci\footcite[p. 27]{flanders_gentle_2019}. Le terme d'enrichissement sémantique, lui, fait référence à l'analyse et au traitement automatisé des données textuelles; il est notamment à l'usage dans le traitement automatisé du langage\footcite{brando_evaluation_2016}. L'enrichissement sémantique se comprend, dans ce projet, comme l'utilisation de sources externes pour intégrer des informations supplémentaires dans les catalogues, et pour faire le lien entre les entrées de catalogues et ces sources externes. L'enrichissement est dit \enquote{sémantique}, puisqu'il s'agit d'ajouter du sens et du contenu dans les catalogues; l'enrichissement automatique fonctionne donc comme un processus d'éditorialisation automatisé des données du projet \mssktb{}. L'éditorialisation automatisée recouvre aussi le troisième terme signifiant du titre, celui de diffusion. Diffuser des données, notamment pour permettre leur réutilisation dans d'autres projets de recherche, demande de réfléchir à la nature des données qui sont distribuées et à la manière dont le texte diffusé peut construit de façon à être, autant que possible, auto-descriptif. Le partage de données à des tiers repose donc sur une réflexion sur les données partagées, ainsi que sur une modélisation de celles-ci qui permette d'inclure les métadonnées suffisantes à leur réutilisation. Il apparaît que, dans un contexte d'humanités, numériques, toutes les étapes de retraitement des données conduisent à une réflexion sur la nature de celles-ci. Aussi les trois termes définis ici ont été utilisés pour structurer l'ensemble du présent mémoire.

J'ai choisi de structurer mon mémoire autour de plusieurs questions connexes, qui, à différents degrés, se retrouvent tout au long du développement:

\begin{adjustwidth}{60pt}{60pt}
	En quoi la nature semi-structurée du corpus permet d'en automatiser le traitement? Comment produire des informations normalisées et exploitables à partir d'un corpus textuel semi-structuré? Comment rendre la recherche en humanités numériques réutilisable et encourager le partage de données entre projets de recherche?
\end{adjustwidth}

La première question est une question de méthode qui offre la colonne vertébrale pour ce mémoire. En effet, le présent texte décrit des méthodes de traitement automatique qui texte qui se basent entièrement sur une connaissance de la structure des documents encodés. La chaîne de traitement s'appuie en effet sur la structure des documents, explicitée par la modélisation des données; mais au delà, ou en deçà de cette structure explicitée par l'encodage, la chaîne de traitement utilise les formulations conventionnelles et la nature répétitive des différentes entrées de catalogue pour manipuler ceux-ci de façon automatisée. Ce mémoire cherche donc à défendre des méthodes alternatives à celles qui sont aujourd'hui utilisées dans le traitement automatisé du langage: alors que ce domaine s'appuie notamment sur l'apprentissage machine et sur d'autres techniques très lourdes à mettre en œuvre, la chaîne de traitement décrite ici est au contraire basée sur des outils techniquement plus simples, mais développées pour s'adapter le mieux possible à un corpus semi-structuré. La seconde question permet de réfléchir à la manière dont le traitement automatisé du texte peut être utilisé dans un projet de recherche. Cette question est surtout pertinente pour la seconde partie de ce mémoire, qui propose une chaîne de traitement visant à produire des données pour une étude économétrique des catalogues de vente. Enfin, la dernière question, concernant le partage de la recherche, est essentielle à la troisième partie de ce mémoire, qui est consacrée à une application pour partager automatiquement des données du projet \mssktb{}.

Ces questions se retrouvent à différents degrés dans les trois parties de ce mémoire. Celui-ci est organisé en suivant les trois termes présentés dans le titre: modélisation, enrichissement sémantique et diffusion. La première partie est consacrée à la production et à la modélisation des données. Elle permet de mieux comprendre la nature et la structure des documents traités, et la manière dont celle-ci peut être mise à profit du traitement automatisé des documents. La seconde partie s'attache à présenter une chaîne de traitement pour l'alignement automatisé des entrées de catalogues avec \wkd{}. Cet alignement s'appuie entièrement sur des méthodes de détection de motifs, et donc sur la nature semi-structurée des documents. Cette chaîne de traitement vise à produire des données qui permettront d'étudier l'impact de facteurs biographiques sur le prix d'un manuscrit; il ne suffit pas pour cela de s'aligner avec \wkd{}, mais il faut également réfléchir à la manière dont cette étude économétrique sera menée. Enfin, la troisième partie s'attache à présenter les objectifs, le fonctionnement et les questionnements soulevés par le développement de \textit{KatAPI}. Cette \gls{api}, qui permet d'automatiser la création et le partage de jeux de données sur mesure, vise à faciliter le partage des données du projet dans un contexte de science ouverte. Cependant, pour permettre ce partage, une réflexion sur la spécificité du texte comme donnée est nécessaire, ainsi qu'une réflexion sur les moyens et enjeux pour la diffusion automatisée du texte. Cette réflexion, et l'outil qui est présenté en troisième partie, visent à apporter une réponse à la question posée en problématique: comment rendre la recherche en humanités numériques réutilisable?

% Les deux premières questions, d'orientation plutôt technique, forment la colonne vertébrale pour le mémoire; elles lient deux aspects centraux: la nature du corpus et la manière dont sa structure permet toute la chaîne de traitement. Par \enquote{semi-structuré}, j'entends que, à un niveau distant, toutes les entrées de catalogue suivent la même structure; des séparateurs distinguent les différentes parties, et les informations sont souvent structurées de manière semblable pour chaque manuscrit vendu. Cela permet un traitement de \enquote{basse technologie} (\emph{low-tech}) en évitant d'entraîner de lourds modèles de traitement du langage naturel (ce qui aboutirait à des solutions complexes, difficiles à maintenir et à faire évoluer et relativement opaques dans leur fonctionnement). À l'inverse, un corpus semi-structuré peut être traité en déduisant une \enquote{structure abstraite}, que chaque entrée de catalogue partage. Il est alors possible de mettre en place des solutions techniques plus faciles, pour un résultat de qualité équivalente. Produire des \enquote{informations normalisées et exploitables} implique de traiter le corpus en cherchant des réponses à des questions de recherche précises -- dans le cadre de mon stage, une question centrale a été de chercher à isoler les facteurs déterminant le prix d'un manuscrit.

% Les deux dernières questions, au premier abord plus théoriques, me semblent centrales, notamment à la troisième partie de ce mémoire. Numérisation, traitement informatisé et diffusion sur le web ne sont pas des opérations neutres, mais un ensemble de \enquote{traductions} des documents originels. Ces processus comportent une part de choix conscients, qu'il s'agit de mettre en avant. Par exemple, on considère que la majorité des documents vendus ont pour titre l'auteur.ice du document. Cette personne n'est cependant pas toujours mentionnée, et des documents peuvent être nommés d'après un lieu, un évènement ou un thème (la Révolution française, par exemple). Ces \enquote{traductions} des catalogues sont relativement discrètes tout au long de la chaîne de traitement (où le format dominant est la \tei{}, qui garde une relation d'équivalence avec le texte). C'est lors du  passage au site web que ce processus de traduction devient plus évident, et, potentiellement, plus problématique. On y abandonne la référence au document originel (les catalogues numérisés ne sont pas accessibles en ligne par un.e utilisateur.ice), le catalogue n'est plus la manière privilégiée d'accéder aux items vendus... De plus, la construction d'un site web implique la conception d'une interface et, dans notre cas, la production d'une série de visualisations intégrées au site. Le passage au site web remet aussi en cause la hiérarchie habituelle entre ingénierie et recherche: la conception d'un site ne répond pas à une question scientifique, mais elle soulève ses propres questions. Loin d'être anodines, ces problématiques de design déterminent la construction et la réception des savoirs. Il est donc important, je pense, de problématiser ces questions de visualisation et de design.