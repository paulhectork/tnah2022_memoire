%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%% DEUXIEME PARTIE %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{Normalisation, enrichissements et extraction d'informations: une chaîne de traitement pour des données semi-structurées}

\chapter{Faire sens d'un corpus complexe: homogénéisation des données et extraction d'informations}
\chaptermark{Faire sens d'un corpus complexe}

\section{Homogénéiser et normaliser un corpus complexe}
Cette section s'intéresse à la manière dont les fichiers \tei{} sont traités afin de pouvoir ensuite en extraire des informations. C'est directement grâce la structure des entrées (et grâce à la nature \enquote{semi-structurée} des catalogues) qu'est possible le traitement automatisé des documents.

\subsection{Pourquoi chercher à normaliser le corpus?}
La question mérite d'être posée: des étapes de post-traitement du corpus sont nécessaires pour pouvoir en extraire des informations et pour pouvoir donc en faire sens; cependant ce processus peut également impacter la nature du texte et ses significations. Tout comme l'édition \tei{} originelle est un processus sélectif, le traitement des documents encodés est lui un processus sélectif: certaines informations contenues par l'encodage sont privilégiées aux dépens d'autres. Il s'agit ici d'expliciter ces choix (travailler sur les prix, les formats et dimensions des manuscrits plutôt que sur leur sujet) et de les justifier. Cette section s'attache donc à rappeler les questions de recherche qui sous-tendent la normalisation des documents (ajouter plus de structure au document \tei{} pour l'exploiter plus facilement, uniformiser la notation des tailles et des dimensions des documents...).

\subsection{Comment normaliser le corpus tout en préservant sa valeur documentaire?}
Ici, on s'intéresse à la manière dont la \tei{} est mise à profit pour enrichir le corpus tout en conservant le contenu textuel des catalogues. Les différentes étapes de normalisation sont également rappelées (ce travail n'étant pas au cœur de mon stage, il s'agira plutôt d'un rappel que d'une présentation technique détaillée).

\section{Faire sens du corpus: extraction d'informations et fouille de texte}
Ici sont décrits le processus et les objectifs de l'extraction d'informations à partir des fichiers \tei{}. C'est à partir de cette opération d'extraction que sont construites les visualisations, qui permettent une approche graphique du corpus et une meilleure compréhension de celui-ci.

\subsection{Extraire des informations au niveau des entrées}
Des données sont extraites pour chaque entrée de catalogue (un travail largement effectué par A. Bartz, que j'ai légèrement mis à jour) : prix (dans la monnaie de l'époque et en francs constants), date de vente normalisée, nom de l'auteur.ice et description du manuscrit... L'extraction d'informations pour les entrées individuelles permet surtout de faire une réconciliation des manuscrits vendus (c'est à dire, de retrouver les items vendus plusieurs fois).

\subsection{Extraire des informations au niveau des catalogues}
Un second processus d'extraction produit des données pour chaque catalogue de vente: titre du catalogue, date de vente, nombre d'items vendus, prix minimum, inférieur et maximum, prix moyen et médian, variance... Ce processus met l'accent sur une approche statistique et économique du corpus, qui permettra d'étudier l'évolution du cours et du volume du marché des manuscrits (nombre d'items en vente, évolution des prix).

\subsection{Vers une approche économique du corpus: la conversion automatique des prix en francs constants}
En même temps que ces processus d'extraction d'informations, un script de conversion des monnaies (françaises et étrangères) en francs constants 1900 a été élaboré. En annulant l'effet de l'inflation, les francs constant permettent d'étudier l'évolution réelle des prix.


\chapter{Vers une étude des facteurs déterminant le prix des documents: alignement des entrées du catalogue avec \wkd{} et exploitation de données normalisées}
\chaptermark{Vers une étude...}
Ce chapitre est construit autour d'une question de recherche: comment produire des informations exploitables pour une étude économétrique à partir d'un corpus textuel semi-structuré? Un des objectifs du projet est de faire l'étude des facteurs déterminant le prix d'un manuscrit. Pour faire cette étude, il faut obtenir, pour chaque entrée du catalogue, un certain nombre d'informations normalisées. Le travail d'extraction de données présentes dans les catalogues a déjà été fait par de précédent.e.s stagiaires. Ces données sont principalement quantitatives: prix des manuscrits, dimensions et nombre de pages, date de création. Il est nécessaire de compléter les informations par des données qualitatives et d'enrichir les données disponibles avec des sources extérieures. Pour ce faire, il a été choisi d'aligner le nom des auteur.ice.s des manuscrits avec des identifiants \wkd{}; dès lors que l'on a un identifiant \wkd{}, il est possible de récupérer automatiquement des informations sur les personnes via \sparql. Le choix de travailler uniquement sur les noms, et non sur la description des documents, a deux motivations:
\begin{itemize}
	\item Les noms de personnes (et la manière dont elles sont décrites) constituent la partie la plus normalisée des documents. La description des manuscrits est plutôt en \enquote{texte libre}. Dans la continuité avec le reste du projet, nous sommes resté dans une approche \enquote{basse technologie}, qui consiste à s'appuyer majoritairement sur des solutions techniquement simples. C'est pourquoi nous avons préférer traiter les noms avec des tables de correspondance\footnote{C'est à dire, des tables qui permettent de normaliser la manière dont les informations figurent dans les catalogues, et donc de remplacer des termes \enquote{vernaculaires} par leurs équivalents utilisés par \wkd{}} et des \rgxpl{}, plutôt que de faire du TAL sur la description des documents.
	\item Toutes les informations \enquote{simples} (données quantitatives facilement normalisables: dates etc.) ont déjà été extraites des descriptions des manuscrits.
\end{itemize}

Ce travail d'enrichissement a été fait en deux temps. 

La première étape, et la plus difficile, est l'alignement avec \wkd{}. Cela demande d'extraire un ensemble d'informations à partir du nom de la personne et de la description de celle-ci. Parmi les informations extraites: nom, prénom, titre de noblesse, occupation, dates de vie et de mort. À partir de ces informations, stockées dans un dictionnaire, un algorithme construit successivement différentes chaînes de caractères pour lancer des recherchers en plein texte sur l'\api{} de \wkd{}. L'objectif est que le premier résultat recherché sur \wkd{} soit correct. Sur un jeu de test, le \gls{score F1} obtenu est de 68\%. Un relecture \enquote{manuelle} des résultats est donc nécessaire.

La deuxième étape, nettement plus simple, consiste à lancer des requêtes \wkd{} sur les identifiants récupérés afin de récupérer des informations sur les auteur.ice.s des manuscrits (cette partie du travail est encore en cours) pour enrichir nos données.

Une fois ce travail effectué, l'enrichissement des données à proprement parler est possible: les fichiers \tei{} sont mis à jour pour ajouter les identifiants \wkd{}. Ainsi, il est possible de faire le lien entre les entrées de catalogues dans des fichiers \xml{} et les données issues de requêtes \sparql{}, stockées dans un \json.


\section{Questions introductives: pourquoi et comment s'aligner avec \wkd{} ?}
\sectionmark{Questions introductives}
Cette section, introductive, répond à des questions évidentes mais essentielles: elles permettent de mettre au clair l'intérêt et les (multiples) difficultés dans l'alignement avec \wkd{}.

\subsection{Pourquoi s'aligner avec des identifiants \wkd{}?} 
% Nos données sont déjà complètes, une pipeline entière existe déjà. Cependant, il peut être difficile de déterminer ce qui fait le prix d'un manuscrit. On aborde les manuscrits avec nos propres catégories intellectuelles du \scl{XXI}, et notre connaissance de l'histoire de l'époque. Il n'est pas non plus possible de reconstruire d'une manière exacte le regard qu'un public du \scl{XIX} aurait sur ces manuscrits -- ce qui permettait de revenir à une perception antérieure de la valeur. Il faut donc chercher à contourner ces biais en produisant des données aussi objectives que possibles. Ainsi, un maximum de variables sont à notre disposition pour calculer des régressions linéaires (qui permettent de prédire l'impact d'une variable sur l'évolution des prix).

L'alignement avec \wkd{} a pour objectif principal de mieux comprendre les déterminants du prix des manuscrits sur le marché du \scl{XIX}. Mais pourquoi passer par un alignement avec \wkd{}? 

Pour étudier les déterminants du prix d'un manuscrit, il faut établir la relation entre la variable dont la valeur est étudiée (le prix d'un manuscrit) et un ensemble d'autres facteurs (qui a écrit un manuscrit, quelles sont ses dimensions, de quand date le document...). En d'autres termes, il faut étudier le comportement d'une variable en fonction d'autres variables. En économétrie, cette opération s'appelle le calcul de régressions linéaires. La variable étudiée (le prix) est dite la variable expliquée; les facteurs déterminant la valeur de cette variable sont dites \enquote{variables explicatives}\footcite{noauthor_regression_2022}. Cependant, cette opération est loin d'être anodine: il faut d'abord identifier les variables pertinentes, et ensuite trouver un moyen de les quantifier. Deux difficultés se présentent pour alors.

Premièrement, il faut pouvoir quantifier les variables expliquées pour calculer des régressions linéaires. Il est possible de leur assigner une valeur numéraire (ce qui est aisé pour les informations quantitatives des catalogues: la date de l'écriture d'un manuscrit, ses les dimensions). Une autre possibilité est de quantifier la présence ou non d'une variable: mention d'un.e destinataire ou du contenu d'un manuscrit. Cependant, ces approches quantitatives ne permettent pas de quantifier des informations complexes, comme la célébrité des auteur.ice.s, ou encore si un manuscrit porte sur un évènement historique ou biographique important (le manuscrit d'un texte célèbre, par exemple, pourrait avoir une valeur particulières). Ces informations sont parfois être présentes dans les catalogues; elles peuvent aussi être connues des lecteur.ice.s d'aujourd'hui et des acheteurs et acheteuses de l'époque. Il n'existe cependant pas de méthodes faciles pour détecter ou quantifier la célébrité d'une personne, ou l'importance d'un sujet.

Une deuxième difficulté découle justement de la part d'implicite qu'il y a dans les catalogues. Les descriptions des items vendus sont brèves, et comprendre ce qui fait la valeur d'un manuscrit demande aux acheteur.euse.s d'avoir des références culturelles et historiques: celles-ci permettent d'identifier l'auteur.ice ou le sujet, et donc pour comprendre la valeur d'un manuscrit. Dans le cadre du projet \mssktb{}, les entrées de catalogues sont traitées par une machine qui, en toute logique, ne dispose pas de ces références. La compréhension qualitative des entrées de catalogues n'est donc pas compatible avec l'approche par lecture distante du projet. Pour éviter de perdre ces informations qualitatives essentielles, il est donc nécessaire de trouver un moyen de quantifier le qualitatif.

En bref, la question est: comment faire la différence entre une lettre de La Rochefoucauld (\ref{fig:rochefoucauld}), vendue 200 francs, et la deuxième (\ref{fig:villars}), vendue à 30 francs? Le problème est un problème de lecture. Une observation de la description des lettres par un être humain comme par une machine peuvent identifier des éléments semblables dans le texte: les deux lettres sont écrites par des ducs; l'une est une \enquote{Très-belle lettre} (\ref{fig:rochefoucauld}), l'autre est une \enquote{Lettre intéressante} (\ref{fig:villars})\footnote{Ce sont des informations qui se retrouvent souvent, et il est donc possible d'écrire un programme qui les relève automatiquement}. Bien que les lettres partagent des attributs, il y a une forte différence de prix entre les deux manuscrits. Un.e lecteur.ice peut trouver une raison à cette différence de prix: La Rochefoucauld et Madeleine de Scudéry n'ont pas le même statut que le duc de Villars. Un regard humain peut donc interpréter un prix et déterminer une valeur en s'appuyant sur ses connaissances. La lecture est qualitative et s'appuie sur de l'implicite, ce qui n'est pas possible pour une machine: formellement, rien ne distingue un nom d'un autre; lorsqu'un programme \enquote{lit} un texte, il ne peut pas s'appuyer sur ses connaissances pour déterminer ce qu'un nom signifie, ce à quoi il fait référence.

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.8\textwidth}
		\includegraphics[width=\textwidth]{img/cat_000372_e24.png}
		\caption{Une lettre écrite par La Rochefoucauld vendue à 200 francs.}
		\label{fig:rochefoucauld}
	\end{subfigure}
	\begin{subfigure}{0.8\textwidth}
		\includegraphics[width=\textwidth]{img/cat_000382_e2158.png}
		\caption{Une lettre écrite par Louis-Hector Villars vendue à 30 francs.}
		\label{fig:villars}
	\end{subfigure}
	\caption{Deux exemples de lettres}
\end{figure}

Pour analyser efficacement la variable \enquote{prix}, il faut pourtant pouvoir, dans une certaine mesure, comprendre les informations implicites et qualitatives contenues dans les catalogues. Le parti pris a donc été de construire le socle de connaissance qui manque à une machine, en s'alignant avec \wkd{} et en s'en servant pour enrichir nos données. Le choix a été fait de ne s'aligner avec \wkd{} que pour certaines parties des entrées de catalogue. Pour rappel, voici leur structure (\ref{code:tei_item}):

\begin{listing}
	\inputminted[linenos, breaklines, tabsize=4]{xml}{code/tei_item.xml}
	\caption{Représentation \xmltei{} d'une entrée de catalogue}
	\label{code:tei_item}
\end{listing}


Les entrées de catalogue contiennent beaucoup d'informations qualitatives, qui pourraient avoir une influence sur le prix du manuscrit: ici par exemple, la description du contenu de la lettre dans le \tnote{}; il est également souvent fait mention du ou de la destinataire. Cependant, l'alignement avec \wkd{} n'a pas été fait avec l'intégralité des entrées. C'est seulement le contenu du \tname{} qui a été aligné avec \wkd{}, à l'aide des informations contenues dans le \ttrait{}. Le \tdesc{} a déjà fait l'objet d'un grand travail de normalisation et d'extraction d'informations; un alignement avec des sources externes n'aurait donc pas une très grande plus-value. L'élément \tnote{} contient souvent des informations intéressantes, puisque c'est là qu'est décrit le contenu d'un manuscrit. Cependant, cet élément n'est pas toujours présent; son contenu est souvent écrit en langage naturel, non structuré, et contient des informations trop variées pour développer un traitement uniforme. Il est donc difficile de tirer parti de cet élément. Le \tname{} et son \ttrait{} sont les éléments les plus régulièrement présents; les informations qu'ils contiennent sont toujours les mêmes (nom d'une personne ou thème d'un manuscrit dans le \tname{}, description du \tname{} dans le \ttrait{}); enfin, ces deux éléments n'ont pas du tout été transformés dans le reste de la chaîne de traitement. Ils portent donc des informations qualitatives centrales pour produire des données exploitables dans une étude économétrique.

Le parti pris a donc été d'aligner avec des identifiants \wkd{} les noms contenus dans les balises \tname{} à l'aide des descriptions contenues dans les \ttrait{}; à partir de cet alignement a été constituée une base de données. Cela permet d'approximer une lecture \enquote{humaine} des items en vente: pour chaque auteur.ice, un certain nombre d'informations auront été récupérées pour mieux identifier la personne (ses occupations, son origine, ses dates de vie...). L'analyse du corpus s'appuit alors sur un bagage de connaissances qui permet d'appréhender par lecture distante l'importance d'une personne. Il devient alors envisageable de voir dans quelle mesure la mention d'une personne impacte le prix d'un manuscrit, et quels sont les facteurs biographiques déterminant dans l'établissement de la valeur. Pour revenir à l'exemple de La Rochefoucauld: à défaut de permettre de savoir qui il est, un alignement avec \wkd{} permet d'identifier son statut et sa place dans la culture française, en récupérant le nombre de ses publications ou encore les institutions dont il est membre.


\subsection{Présentation générale de l'algorithme}
Construire un jeu de données issu de \sparql{} à partir de la manière dont une personne est nommée et décrite au \scl{XIX} n'est pas une opération anodine. La chaîne de traitement est donc assez complexe, comme le montre le schéma \ref{fig:wkdmain}. Cette chaîne de traitement peut être séparée en trois étapes.

\subsubsection{Étape 1 -- Extraction et structuration de données}
Premièrement, il s'agit d'aligner les entrées de catalogue avec des identifiants \wkd{}. Ceux-ci sont liés à des \enquote{entités} \wkd{}: des personnes, lieux et évènements décrits dans \wkd{} par un certain nombre de propriétés (date de naissance, lieu de résidences...). Cette première étape repose avant tout sur l'extraction et la traduction des données depuis les éléments \tname{} et \ttrait{}. Ce processus d'extraction permet de récupérer toutes les données pertinentes pour chaque entrée de catalogue et de les stocker dans un \gls{dictionnaire} structuré. Comme on le verra, la nature \enquote{semi-structurée} des entrées (ainsi qu'une bonne connaissance du corpus) permet de d'automatiser le processus d'extraction et de traduction des données par détection de motifs, sans avoir à passer par l'apprentissage machine: étant donné que les mêmes types d'informations sont toujours présentes et que les entrées suivent des modèles relativement proches, il est possible de s'appuyer sur la structure des entrées pour identifier  les informations pertinentes. L'extraction de données repose donc sur de la détection de motifs à l'aide d'\rgxpl{}: des récurrences sont repérées dans le texte et utilisées pour distinguer différentes informations (nom, prénom, titre de noblesse...). Pour appuyer l'usage d'expressions régulières par une méthode plus \enquote{qualitative} et précise, certains termes particuliers sont extraits et éventuellement traduits à l'aide de \glspl{table de conversion} (c'est-à-dire de \glspl{dictionnaire} qui associent à un terme dans le texte une version normalisée). 

\subsubsection{Étape 2 -- récupération d'identifiants \wkd{} via des recherches en plein texte à l'aide d'une \api{}}
Une fois les données du \tname{} et du \ttrait{} structurées en dictionnaire, elles sont utilisées pour lancer plusieurs recherches en plein texte sur le moteur de recherche de \wkd{}. Ces recherches sont faites automatiquement grâce à l'\api{} de \wkd{}. Pour maximiser les chances d'obtenir un identifiant valide, un algorithme a été conçu pour lancer plusieurs recherches à partir de chaque dictionnaire. La première recherche met bout-à-bout toutes les valeurs disponibles dans le dictionnaire. Ensuite, en fonction des paramètres de recherche disponibles dans le dictionnaire, différentes autres recherches sont lancées. Cet algorithme a été élaboré en menant de nombreux tests pour maximiser le taux de réussite, calculé sous la forme d'un \gls{score F1}.

\subsubsection{Étape 3 -- constitution d'un jeu de données à l'aide de \sparql{}}
Si l'étape d'alignement avec \wkd{} est la plus complexe, elle n'est qu'une étape préparatoire vers la constitution du jeu de données. En fait, récupérer les identifiants est seulement ce qui rend possible l'enrichissement en tant que tel: en lançant une requête \sparql{} sur tous ces identifiants, il est possible, pour chaque entité représentée par l'identifiant, de récupérer des informations depuis \wkd{} et donc de construire le jeu de données définitif. Pour cette étape, le processus est plus simple: les identifiants récupérés à la fin de l'étape précédente sont dédoublonnés (pour éviter de lancer plusieurs fois la même requête); ensuite une requête \sparql{} est initalisée et lancée chacun des identifiants. Les résultats sont traduits depuis les formats \json{} ou \xml{} retournés par \sparql{} sous forme de \json{} plus simple, et donc plus aisément manipulable. Le jeu de données est enregistré dans un fichier. Pour finir, les identifiants \wkd{} sont réinjectés aux catalogues \tei{}, afin de pouvoir faire le lien entre les catalogues et le jeu de données qui a été construit.

Cette chaîne de traitement étant lancée sur plus de 80000 entrées de catalogues, le temps d'exécution est très long et même des petites améliorations de performance peuvent avoir un grand impact; dans sa version initiale, le script demandait des performances particulièrement élevées, et ne fonctionnait pas sur un ordinateur aux capacités limitées. La chaîne de traitement a donc été reprise en plusieurs points afin d'être optimisée, de fonctionner plus vite en étant moins coûteuse en ressources.

\widepage
\begin{figure}[!p]
	\centering
	\tikz[node distance=1cm, scale=0.85, transform shape]{
		\node[db] %
		(db1) at (-5,0)%
		{Source: tableur contenant les \tname{} et \titem{}};
		\node[base]%
		(start) at (0,0)%
		{Lancement de l'algorithme sur toutes les entrées du tableur};
		\draw[arrow] (db1) -- (start);
		
		\node[choice]%
		(detect) at (0,-3.5)%
		{Étape 1 -- extraction de donnée des \tname{} et \ttrait{}};
		\draw[arrow] (start) -- (detect);
		
		\node[transf]%
		(name) at (-4,-6.5)%
		{Extraction et normalisation d'informations nominatives du \tname{}};
		\node[transf]%
		(trait) at (4,-6.5)%
		{Extraction et normalisation d'informations biographiques du \ttrait{}};
		\node[base]%
		(dict) at (0,-9.5)%
		{Constitution d'un dictionnaire structuré pour aligner les \tname{} avec des entités \wkd{}};
		\draw[arrow] (detect) -- (name);
		\draw[arrow] (detect) -- (trait);
		\draw[arrow] (name) -- (dict);
		\draw[arrow] (trait) -- (dict);
		
		\node[db]%
		(logs) at (6,-13)%
		{Optimisation: enregistrement des entrées de catalogues et requêtes déjà traitées};
		\node[transf]%
		(api) at (0,-13)%
		{Étape 2 -- Alogrithme de requêtes sur l'API \wkd{} pour récupérer des identifiants};
		\draw[arrow] (api) -- (logs);
		\draw[arrow] (dict) -- (api);
		
		\node[db]%
		(idset) at (0,-17.5)%
		{Liste d'identifiants \wkd{} à partir de laquelle est constitué le jeu de données};
		\node[transf]
		(sparql) at (0,-20.5)
		{Étape 3 -- Requêtes \sparql{}};
		\node[base]%
		(end) at (0, -23)%
		{Conversion des résultats en \json{} et enregistrement};
		\node[db]%
		(sparqldata) at (5,-23)%
		{Jeu de données issues de \sparql{} au format \json{}};
		\node[transf]%
		(tei) at (0,-25.5)%
		{Réinjection des identifiants \wkd{} dans les fichiers \tei{}};
		\node[db]%
		(teidb) at (-5,-25.5)%
		{Corpus de catalogues en \tei{} augmentés des identifiants \wkd{}};
		\draw[arrow] (api) -- (idset);
		\draw[arrow] (idset) -- (sparql);
		\draw[arrow] (sparql) -- (end);
		\draw[arrow] (end) -- (sparqldata);
		\draw[arrow] (end) -- (tei);
		\draw[arrow] (tei) -- (teidb);
	}
	\caption{\\Présentation générale de l'algorithme d'enrichissement de données à l'aide de \wkd{}}
	\label{fig:wkdmain}
\end{figure}
\restoregeometry

\subsection{Comment traduire des descriptions textuelles datant du XIX\up{ème}~s. en chaînes de caractères qui puissent retourner un résultat sur \wkd{}?} 
Dans la réalisation de cet algorithme, la principale difficulté porte sur la récupération d'identifiants \wkd{} à l'aide de recherches en plein texte. Le script prend en entrée un nom et sa description -- tels qu'elles figurent dans des catalogues datant majoritairement du \scl{XIX}. La difficulté, au delà de la détection et de l'extraction d'informations, est de traduire ces informations pour qu'elles permettent de trouver des résultats pertinents sur \wkd{}. Ce problème est autant linguistique de technique. Une personne ou une chose est nommée ou décrite d'une certaine manière dans un catalogue de vente ancien. Il n'y a aucune garantie que cette caractérisation corresponde à celle faite par \wkd{}: l'orthographe des noms évoluent, tout comme la manière de nommer certains métiers. À ces évolutions orthographiques s'ajoutent des évolutions intellectuelles: les titres de noblesse sont un marqueur plus important au \scl{XIX} français que dans un \scl{XXI} mondialisé. Une personne n'est que rarement décrite par son titre dans \wkd{}. 

\subsubsection{Le problème de la traduction des noms}
Il existe bien sûr des cas simples, comme l'exemple \ref{code:name1}: en extrayant le contenu du \tname et en traduisant le \enquote{roi} issu du \ttrait{}, la chaîne de caractère obtenue est \enquote{Henri IV king}. En recherchant cette chaîne de caractère sur \wkd{}, \href{https://www.wikidata.org/w/index.php?search=Henri+IV+king\&title=Special:Search\&profile=advanced\&fulltext=1\&ns0=1\&ns120=1}{le premier résultat obtenu est correct}. Cependant, de nombreux cas sont plus complexes, surtout lorsque l'auteur.ice du manuscrit est moins célèbre. L'exemple \ref{code:name2} est éclairant: dans le catalogue, la personne est nomée \enquote{Bruno Daru}; sur \wkd{}, le nom de la personne est \enquote{Pierre Daru}, et son nom complet {Pierre Antoine Noël Bruno Daru}. Si la recherche en plein texte est faite avec les mêmes paramètres que pour l'exemple précédent (nom de la personne et titre de noblesse), \href{https://www.wikidata.org/w/index.php?search=bruno+daru+count&title=Special:Search&profile=advanced&fulltext=1&ns0=1&ns120=1}{le premier résultat obtenu} n'est pas le bon: c'est un renvoi à un article de \textit{l'Encyclopedia Britannica} datant de 1911. C'est en cherchant seulement le nom est le prénom que \wkd{} retourne \href{https://www.wikidata.org/w/index.php?search=bruno+daru&title=Special:Search&profile=advanced&fulltext=1&ns0=1&ns120=1}{un résultat pertinent}. Il est intéressant de retenir deux choses de cet exemple: dans les catalogues, le prénom d'une personne correspond en fait souvent à son deuxième ou troisième prénom; ensuite, le titre de noblesse est un critère plus fréquemment mentionné dans les catalogues que dans \wkd{}. Cela s'explique assez aisément: le \scl{XIX} connaît une alternance de régimes politiques (royauté, empire, république) où la noblesse n'a pas encore perdu son pouvoir. La probabilité qu'un titre de noblesse soit mentionné sur \wkd{} diminue lorsqu'un titre est peu important; dans les catalogues, cependant, même les titres les moins importants sont régulièrement mentionnés. Par conséquent, seuls les titres les plus importants seront extraits pour lancer une recherche sur l'\api{} de \wkd{}.

\begin{listing}
	\begin{minted}{xml}
<item n="134" xml:id="CAT_000233_e134">
	<!-- ... -->
	<name type="author">Henri IV</name>
	<trait>
		<p>roi de France.</p>
	</trait>
	<!-- ... -->
</item>
	\end{minted}
	\caption{Un cas simple: Henri IV roi de France}
	\label{code:name1}
\end{listing}

\begin{listing}
	\begin{minted}{xml}
<item n="98" xml:id="CAT_000082_e98">
	<!-- ... -->
	<name type="author">Daru (Bruno, comte)</name>
	<trait>
		<p>célèbre ministre de Napoléon Ier, historien de Venise, de l'Acad. fr., né à Montpellier</p>
	</trait>
	<!-- ... -->
</item>
	\end{minted}
	\caption{Un cas plus complexe: Pierre Antoine Noël Bruno Daru}
	\label{code:name2}
\end{listing}

Dans le cas de noms de personnes étrangères, la situation peut être plus complexe encore. L'exemple \ref{code:name3} combine différentes difficultés.
\begin{itemize}
	\item D'abord, la personne est étrangère; dans les catalogues, les noms sont systématiquement françisés -- \enquote{Albert-Venceslas-Eusèbe} dans le catalogue, \enquote{Albrecht Wenzel Eusebius} en langue originelle. Se pose donc la question de si le nom doit être traduit, et si oui comment? 
	\item Ensuite, comme l'indique la présence de \enquote{dit} dans le \tname{}, il est mentionné un nom de naissance (\enquote{de Waldstein}) et un nom d'usage (\enquote{Wallenstein}). Idéalement, il faudrait choisir entre l'un ou l'autre, plutôt que de rechercher \enquote{Waldstein Wallenstein} sur \wkd{}, ce qui risque d'augmenter le bruit. 
\end{itemize}

Notre approche s'appuyant sur la structure du texte, le deuxième point peut être réglé: le nom d'usage est écrit au début, et le nom de naissance entre parenthèses (c'est également le cas des noms de personnes nobles, par exemple). Il est donc possible de choisir l'un ou l'autre nom. Le premier point est plus problématique: si la traduction du nom serait envisageable en théorie, celle-ci est difficilement compatible avec une approche basée sur la détection de motifs dans le texte: le prénom est repérable comme étant un motif (trois noms séparés par des tirets); cependant, il est impossible de le traduire automatiquement (ce qui demanderait de connaître la langue dans laquelle un prénom doit être traduit). C'est ici que les informations contenues dans le \ttrait{} prennent leur importance: lorsqu'il y a des défaillances dans les informations nominatives, des données biographiques permettent de diminuer le risque d'erreurs. Dans cet exemple, recherche \enquote{Albert-Venceslas-Eusèbe Waldstein} ne \href{https://www.wikidata.org/w/index.php?search=Albert-Venceslas-Eus%C3%A8be+de+Waldstein&title=Special:Search&profile=advanced&fulltext=1&ns0=1&ns120=1}{retourne aucun résultat}, de même que rechercher \href{https://www.wikidata.org/w/index.php?search=Albert-Venceslas-Eus%C3%A8be+Wallenstein&title=Special:Search&profile=advanced&fulltext=1&ns0=1&ns120=1}{Albert-Venceslas-Eusèbe Wallenstein}. Cependant, le bon résultat est obtenu en recherchant \href{https://www.wikidata.org/w/index.php?search=Wallenstein+1634&title=Special:Search&profile=advanced&fulltext=1&ns0=1&ns120=1}{\enquote{Wallenstein 1634}}. Une difficulté supplémentaire vient avec ce type de cas: différents paramètres de recherche (nom, prénom...) ont un impact différent dans l'obtention du bon résultat en fonction des personnes sur qui la requête est faite. Dans ce cas, rechercher le nom d'usage et la date de naissance retourne un résultat valide, ce qui n'est pas toujours le cas. Pour contourner ce problème, trois solutions ont été mises en place: d'abord, ce types de requêtes a été fait \enquote{à la main}, de façon non-automatique, pour de nombreuses entrées différentes afin de déterminer la meilleure combinaison de caractères; ensuite, des tests qui permettent de mesurer l'influence de chaque paramètre de recherche dans l'obtention du résultat; enfin, l'algorithme final lance successivement différentes requêtes avec différents paramètres afin de maximiser la probabilité d'obtenir un résultat valide. Nous reviendrons plus en détail sur les deux derniers points.

\begin{listing}
	\begin{minted}{xml}
<item n="5518" xml:id="CAT_000401_e5518">
	<!-- ... -->
	<name type="author">Wallenstein (Albert-Venceslas-Eusèbe de Waldstein  dit)</name>
	<trait>
		<p>duc de Friedland, célèbre général de la guerre de Trente ans. Assassiné en 1634.</p>
	</trait>
	<!-- ... -->
</item>
	\end{minted}
	\caption{Le problème des noms de personnes étrangères}
	\label{code:name3}
\end{listing}

\subsubsection{L'extraction d'informations biographiques: une autre difficulté}
Cependant, le problème ne s'arrête pas qu'aux noms. Dans un exemple; précédent, le titre de noblesse influençait l'obtention d'un résultat valide. De nombreuses autres informations biographiques pourraient, au premier abord, permettre d'obtenir le bon résultat. C'est souvent le cas, puisque extraire le métier ou la fonction d'une personne permet de supprimer les faux positifs retournés par l'\api{}. C'est par exemple le cas dans l'exemple \ref{code:bio1}. En cherchant uniquement le nom et le prénom (\enquote{Hans Bulow}), le premier résultat retourné renvoie \href{https://www.wikidata.org/w/index.php?search=hans+bulow&title=Special:Search&profile=advanced&fulltext=1&ns0=1&ns120=1}{à un journaliste suédois}. Extraire le mot \enquote{pianiste} \ttrait{} et le traduit en anglais permet d'obtenir \href{https://www.wikidata.org/w/index.php?search=hans+bulow+pianist&title=Special:Search&profile=advanced&fulltext=1&ns0=1&ns120=1}{le bon résultat}.

\begin{listing}
	\begin{minted}{xml}
<item n="136" xml:id="CAT_000189_e136">
	<!-- .. -->
	<name type="author">Bulow (Hans)</name>
	<trait>
		<p>le célèbre pianiste.</p>
	</trait>
</item>
	\end{minted}
	\caption{Un exemple où l'extraction du métier permet l'obtention du bon résultat}
	\label{code:bio1}
\end{listing}

L'extraction d'informations biographiques et leur utilisation dans des requêtes est donc pertinent. Cependant, extraire trop d'informations conduit à lancer des requêtes qui ne renvoient aucun résultat. Dans les exemples \ref{code:bio2} et \ref{code:bio3}, extraire et traduire des fonctions conduit à lancer les requêtes \href{https://www.wikidata.org/w/index.php?search=John+Okey+colonel&title=Special:Search&profile=advanced&fulltext=1&ns0=1&ns120=1}{\enquote{John Okey colonel}} et \href{https://www.wikidata.org/w/index.php?search=Jean+Bouhier+president&title=Special:Search&profile=advanced&fulltext=1&ns0=1&ns120=1}{\enquote{Jean Bouhier president}} qui ne retournent aucun résultat, ou des résultats qui ne sont pas valides. Cependant, dans les deux cas, si une requête est lancée sans la fonction, un résultat correct est obtenu. Les raisons pour lesquelles des résultats erronés sont retournés ne sont cependant pas les mêmes, et il est intéressant de mieux observer les requêtes lancées et les résultats obtenus. Dans le premier cas, le terme mis en avant dans le \ttrait{} (\enquote{colonel}) n'est pas celui avec lequel la personne est décrite sur \wkd{} (où John Okey est décrit comme étant un homme politique). Cela met en avant un problème relatif au changement de regard sur des personnalités: dans un contexte, la personne est décrite comme une figure militaire, dans l'autre comme une figure politique. Le deuxième cas est plus technique. Il y a en fait une erreur dans la requête qui ne retourne pas de résultat (\enquote{Jean Bouhier president}): un.e président.e de parlement n'est en général pas décrite comme \enquote{président}. Cependant, en extrayant des données uniquement par détection de motifs, il est possible de repérer et traduire un terme générique comme \enquote{président}. Extraire le complément \enquote{Parlement de Dijon} du \ttrait{} n'est cependant pas possible (cela impliquerait d'étudier la grammaire de la phrase, pour mettre en avant la relation entre \enquote{président} et \enquote{Parlement de Dijon}). Au vu de la taille et de la variété du jeu de données, il est impossible de traiter au cas par cas des entrées, ou préciser la détection de motif avec suffisamment de précision pour pouvoir résoudre ce genre de difficultés. 

De situations comme les exemples \ref{code:bio2} et \ref{code:bio3}, il faut donc retenir que l'extraction d'informations vient nécessairement avec un risque d'erreur. Le parti pris a donc été de ne pas repérer les métiers et autres termes très spécifiques, comme les grades militaires et les titres de noblesse peu élevés: ils ne retournent pas de résultats sur le moteur de recherche. Ensuite, plus des requêtes sont précises, plus elles risquent de retourner du silence (c'est-à-dire, de ne pas donner de réponse); cependant, si un résultat est obtenu, il est plus probable que ce résultat soit correct. Une fois l'extraction d'informations faite, l'algorithme d'extraction d'identifiants sur l'\api{} \wkd{} a donc été conçu en suivant un principe soustractif: les premières recherches sont faites avec un maximum de paramètres; si aucun résultat n'est obtenu, des paramètres sont enlevés pour que l'\api{} retourne un plus grand nombre de résultats. Enfin, ces deux exemples montrent qu'il n'est pas possible d'extraire et de traduire des informations sans prendre en compte ce qui sera pertinent pour le moteur de recherche de \wkd{}. Il ne s'agit donc pas seulement d'extraire des informations, mais aussi de s'adapter avec ce moteur de recherche pour augmenter la probabilité d'obtenir un résultat valide.

\begin{listing}
	\begin{minted}{xml}
<item n="152" xml:id="CAT_000189_e152">
	<!-- ... -->
	<name type="author">Okey (John)</name>
	<trait>
		<p>colonel anglais, un des lieutenants de Cromwell.</p>
	</trait>
	<!-- ... -->
</item>
	\end{minted}
	\caption{Quand l'extraction d'un métier conduit à des requêtes trop spécifiques}
	\label{code:bio2}
\end{listing}

\begin{listing}
	\begin{minted}{xml}
<item n="5430" xml:id="CAT_000401_e5430">
	<!-- ... -->
	<name type="author">Bouhier (Jean)</name>
	<trait>
		<p>président au Parlement de Dijon, membre de l'Académie française.</p>
	</trait>
	<!-- ... -->
</item>
	\end{minted}
	\caption{Le cas des métiers dont l'extraction est problématique}
	\label{code:bio3}
\end{listing}

% Il faudra donc mettre en place un processus de traduction, depuis le langage naturel en français du \scl{XIX} en anglais adapté aux termes utilisés dans \wkd{}.

\subsection{Comment négocier avec le moteur de recherche de \wkd{}?}
Comme cela commence à apparaître, l'extraction d'informations, lorsqu'elle vise à interagir avec des données externes, vient avec des difficultés supplémentaires. Il ne faut pas seulement extraire les informations ; leur extraction et structuration doivent permettre de lancer des recherches en plein texte, et donc de minimiser le bruit (les informations non pertinentes) et le silence (l'absence d'informations) de la part du moteur de recherche. Il faut donc traduire les informations extraites pour qu'elles correspondent au vocabulaire utilisé par \wkd{}. Cette opération n'est pas anodine: si les catalogues de vente fonctionnent avec leurs propres catégories, le même peut être dit de \wkd{}: certains types de données sont plus souvent référencées que d'autres et \wkd{} utilise un vocabulaire qui lui est propre. Pour bien mener ce processus de traduction et de structuration de l'information, il est nécessaire de bien connaître le fonctionnement de ce moteur de recherche pour mieux s'y adapter.

Comme cela a été dit, l'alignement avec \wkd{} passe par l'utilisation de l'\api{} mise en point par l'institution afin de lancer automatiquement des recherches en plein texte; l'objectif est que le premier résultat retourné par le moteur de recherche soit le bon. La première chose à remarquer est que, contrairement à un moteur de recherche généraliste (comme \textit{Google, QWant}...), ce moteur n'est pas compatible avec des requêtes approximatives. L'exemple \ref{code:search1} est pertinent à ce égard\footnote{Dans cet exemple, le prénom, \enquote{M.-D.-A.}, n'est pas pris en compte pour se concentrer sur l'utilisation d'informations biographiques dans le \ttrait{}.}. Dans de nombreuses entrées, comme c'est le cas ici, les fonctions d'une personne ayant participé à la révolution sont présentées de façon précise: Marc David Alba Lasource est décrit comme étant un \enquote{conventionnel girondin}. Cette mention, régulièrement présente dans les catalogues, pourrait être relevée en tant que telle. Cependant, lancer la recherche \enquote{Lasource conventionnel} ne retourne \href{https://www.wikidata.org/w/index.php?search=lasource+conventionnel&title=Special:Search&profile=advanced&fulltext=1&ns0=1&ns120=1}{aucun résultat}. Si la même recherche est lancée sur un moteur de recherche généraliste (ici, \textit{QWant}), la page \textit{Wikipedia} de Lasource fait partie des premiers \href{https://www.qwant.com/?q=lasource+conventionnel}{résultats}\footnote{Le 29/07/2022, c'est le troisième résultat; le premier correspond à une vente aux enchères d'archives du conventionnel. Les moteurs de recherche pouvant être mis à jour régulièrement, il est possible que l'ordre des résultats change}. Cette différence dans les données retournées par les moteurs de recherche a deux explications: un moteur de recherche généraliste recherche les occurrences de mots, non seulement dans le titre de la page, mais aussi dans le corps du texte. Si le mot \enquote{conventionnel} est absent du titre, il est certainement à plusieurs reprises dans une notice biographique type \textit{Wikipedia}. \wkd{} ne contenant que des données, et pas de texte en tant que tel, l'indexation du corps du texte par le moteur de recherche interne à \wkd{} n'est pas possible. Ensuite, la plupart des moteurs de recherche généralistes utilisent des méthodes de traitement du langage afin de simplifier la requête lancée par l'utilisateur.ice: les mots recherchés sont simplifiés, le moteur de recherche associe les termes recherchés avec d'autres termes \enquote{cooccurents}, c'est-à-dire fréquemment utilisés ensemble\footnote{\cite{noauthor_moteur_2022}. Pour des analyses plus détaillées sur la construction d'ensemble de termes coocurrents via le développement de vecteurs de mots, voir \cite{mikolov_efficient_2013}; pour un article technique détaillant la classification et la sélection de résultats pertinents par apprentissage profond, voir \cite{covington_deep_2016}}. Dans le cas du moteur de recherche de \wkd{}, la requête de l'utilisateur.ice ne semble pas être retraitée: des signes de ponctuation ou des fautes de frappes influencent l'obtention d'un résultat, de même que l'usage de termes inadaptés.

Pour faire face à la \enquote{rigidité} relative du moteur de recherche de \wkd{}, il est donc nécéssaire de préparer ses données au moment de leur extraction. En prenant le même exemple (\ref{code:search1}), un résultat correct peut être obtenu en remplaçant \enquote{conventionnel} par \enquote{politician}\footnote{\enquote{Personnalité politique}}, pour rechercher sur \wkd{} \href{https://www.wikidata.org/w/index.php?search=lasource+politician&search=lasource+politician&title=Special%3ASearch&go=Go&ns0=1&ns120=1}{\enquote{Lasource politician}}. Ici, la traduction de \enquote{conventionnel} en \enquote{politician} est d'autant plus intéressante que la date de naissance dans le catalogue (1762) ne correspond pas à celle indiquée sur \wkd{} (1763). Dans un cas comme celui-ci, où certaines données sont incorrectes, il est important d'extraire un maximum d'informations pour que, si certaines requêtes ne rapportent pas de résultats, pouvoir en faire d'autres avec différents paramètres.

\begin{listing}
	\begin{minted}{xml}
<item n="140" xml:id="CAT_000197_e140">
	<!-- ... -->
	<name type="author">Lasource (M.-D.-A.)</name>
	<trait>
		<p>célèbre conventionnel girondin, né près de Montpellier en 1762, guillotiné en 1793.</p>
	</trait>
	<!-- ... -->
</item>	
	\end{minted}
	\caption{Le problème de l'approximation et de la traduction: Lasource, conventionnel}
	\label{code:search1}
\end{listing}

En conclusion, il faut retenir que le moteur de recherche de \wkd{} n'admet pas d'erreurs, ni de requêtes partiellement erronées (dans l'exemple \ref{code:search1}, où la date de naissance soit correcte, mais pas la date de mort); il ne prend pas non plus en compte la synonymie, ce qui veut dire qu'il n'améliore pas la requête lancée par un.e utilisateur.ice... Cela signifie que les termes utilisés dans une requête doivent être adaptés à ceux que \wkd{} utilise. Les termes spécifiques utilisés dans les catalogues (\enquote{conventionnel}), mais aussi de nombreux titres militaires et de noblesse peu élevés (\enquote{capitaine}, \enquote{marquis}) sont relativement rarement présents sur \wkd{}. Lorsque les requêtes sont lancées, de tels termes sont donc abandonnés et parfois remplacés par des termes plus génériques: par exemple, \enquote{capitaine} est remplacé par \enquote{military}, traduction anglaise de \enquote{militaire}. De même, des termes principalement en usage dans la langue française, comme \enquote{conventionnel} sont moins efficaces pour lancer des recherches.

\subsection{Une approche prédictive}
L'alignement avec \wkd{} et l'extraction d'entités n'est donc pas une opération anodine: les données contenues dans les catalogues sont variées, autant par leur structure que par les informations qu'elles contiennent; il peut être difficile à faire la traduction de données du \scl{XIX} en chaînes de caractères pouvant retourner des réponses valides sur \wkd{}; enfin, le l'alignement repose sur une bonne connaissance du moteur de recherche de \wkd{}. 

De plus, la technique utilisée dans l'extraction de données, reposant sur la détection de motifs à l'aide de \glspl{expression régulière} et de \glspl{table de conversion}, est une technique qui vient avec un certain nombre d'incertitudes. Avec ce genre de techniques, il est impossible de \enquote{comprendre} ce qu'un élément signifie. Dans l'exemple \ref{code:predic}, formellement, rien ne sépare le nom propre de la duchesse (\enquote{Séguier}) du nom de son duché (\enquote{Verneuil}). En s'appuyant sur une connaissance de la structure répétitive des entrées, il est uniquement possible de supposer que le nom entre parenthèses est un nom propre, tandis que le nom hors des parenthèses correspond au nom du duché. En bref, les méthodes de détection de motifs utilisées, peuvent uniquement inférer le sens d'un mot par rapport à sa position dans une phrase. Si cette technique implique une certaine incertitude, elle est cependant particulièrement adaptée à un corpus semi-structuré, comme c'est le cas des catalogues de vente de manuscrits, et à l'opération d'alignement avec \wkd{}. Comme cela est expliqué plus bas, au fond, il n'est pas tellement important de distinguer le sens des différentes informations: ce qui a du sens, c'est que l'extraction et la structuration des informations permet de construire des chaînes de caractères à rechercher sur \wkd{}. Identifier la fonction d'un mot n'est donc ici qu'un moyen -- contrairement à de l'analyse lexicale, où la fonction des mots dans une phrase est signifiante --. En effet, repérer le rôle que tiennent les termes extraits (métier, prénom...) permet de mieux construire la chaîne de caractère recherchée sur \wkd{}, en pouvant filtrer certaines informations (retirer les dates de vie et de mort, par exemple).

\begin{listing}
	\begin{minted}{xml}
		<name type="author">Verneuil (Charlotte Séguier duchesse de)</name>
	\end{minted}
	\caption{Peut-on identifier les différents éléments d'une phrase par détection de motifs?}
	\label{code:predic}
\end{listing}

Étant donnée cette quantité d'incertitudes, l'approche suivie dans l'alignement avec \wkd{} peut être qualifiée de \enquote{prédictive}. Par ce terme, il faut comprendre que il n'y a pas, de certitude totale dans le processus d'extraction et de traduction des données. Il n'est pas possible de récupérer avec une certitude totale le bon identifiant. L'objectif cet algorithme n'est donc pas de trouver la \enquote{bonne} réponse. Il est de construire une chaîne de caractère dont on prédit qu'elle apportera un résultat pertinent. De la même manière, la phase de préparation des données est un processus qui sélectionne et normalise certaines informations dont on considère -- après un long processus de test et d'essais -- qu'elles seront pertinentes dans l'obtention des bons résultats. Enfin, le premier rôle des tests est de quantifier les prédictions. Ils répondent à la question: étant donné les résultats obtenus lors des tests, quelle est la probabilité que la prochaine chaîne de caractères recherchée retourne un résultat pertinent? Cette approche prédictive implique nécessairement un degré d'incertitude, et donc le développement d'algorithmes flexibles qui cherchent à minimiser le bruit. 

Être conscient de la nature prédictive de ce processus et quantifier la qualité des algorithmes à l'aide de tests permet cependant de prendre de meilleures décisions techniques. La lecture distante et la détection de motifs supposent d'avancer \enquote{à l'aveugle}, en s'appuyant sur sa connaissance de la structure du texte pour extraire les bonnes informations. Étant donné qu'il est impossible d'être totalement certain que les bonnes données ont été extraites, l'étape suivante -- le lancement des requêtes sur l'\api{} -- doit malléable et s'adapter aux données disponibles. C'est pourquoi le parti pris a été de concevoir un algorithme qui continue de lancer des requêtes en retirant des paramètres tant qu'un identifiant n'a pas été trouvé.

\section{Un algorithme de détection de motifs pour préparer et structurer les données}
\sectionmark{Un algorithme de détection de motifs...}
Avant de chercher à récupérer un identifiant \wkd{} via l'\api{}, un algorithme se charge de traduire et de structurer les données: à partir d'un nom et de son éventuelle description, un dictionnaire qui contient les informations de manière structurée est construit. Cette étape était initialement censée être une simple extraction d'information: à partir du \tname{} et du \ttrait{}, un ensemble d'informations étaient mises bout à bout afin de former une chaîne de caractères à rechercher sur l'\api{}. Le processus s'est complexifié pour intégrer l'extraction, la traduction et la structuration des données. En construisant un \gls{dictionnaire} à partir de texte, il est possible de savoir précisément quelles données sont disponibles pour lancer des requêtes; plusieurs requêtes peuvent alors être lancées sur l'\api{} avec différents paramètres, ce qui permet d'augmenter les probabilités d'obtenir un identifiant valide. 

\subsection{Présentation générale}
\subsubsection{Les formats d'entrée et de sortie}
Le but de l'extraction de données permet de transformer la représentation \tei{}  visible en \ref{code:prepin} -- représentée sous forme d'un \tsv{} pour faciliter la lecture des données -- au \gls{dictionnaire} visible en \ref{code:prepout}. Voici la significtion des différentes clés\footnote{Une clé de \gls{dictionnaire} est l'élément à gauche des \enquote{:}; la clé permet d'accéder à la valeur, visible à droite du \enquote{:}, ce qui permet d'associer des valeurs entre elles, et donc de stocker des objets ou de remplacer une clé présente dans un texte par une valeur, par exemple.} du format de sortie:
\begin{itemize}
	\item \texttt{fname}: cette clé permet d'accéder au prénom d'une personne. Les données contenues dans cette clé viennent du \tname{}. \texttt{fname} est l'abréviation de \enquote{first name}.
	\item \texttt{lanme}: cette clé permet d'accéder au nom de famille de quelqu'un. C'est cette information, extraite du \tname{}, qui est centrale aux requêtes. \texttt{lname} abréviation de \enquote{last name})
	\item \texttt{nobname\_sts}: cette clé contient un nom de famille noble. Dans ces cas, un titre de noblesse est présent dans les entrées de catalogue (seuls les titres de noblesse les plus importants sont extraits du dictionnaire, ce qui n'est pas le cas ici). Les informations contenues ici proviennent du \tname{}. Cette clé est l'abréviation de \enquote{nobility name\_status}
	\item \texttt{status}: le statut d'une personne, soit son titre de noblesse (ici, le titre \enquote{vicomte} n'a pas été extrait car il est rarement présent sur \wkd{}). Les informations contenues ici proviennent en général du \tname{} et parfois du \ttrait{}.
	\item \texttt{dates}: les dates de naissance ou de mort d'une personne (seules ces dates sont conservées). Ces informations proviennent du \ttrait{}.
	\item \texttt{function}: la fonction d'une personne, soit, en général, son métier ou son occupation principale. Cette information provient du \ttrait{}.
	\item \texttt{rebuilt}: un booléen indiquant si un prénom a été reconstruit à partir d'initiales ou non.
\end{itemize}

Comme cela a été dit auparavant, le nom attribué à ces clés n'est pas systématiquement indicateur des valeurs qui y sont associées: si l'entrée de catalogue correspond à une personne, alors les clés correspondent aux informations qu'elles contiennent. Si l'entrée de catalogue ne correspond pas à une personne, ces clés seront également utilisées. Ce qui est important, c'est la hiérarchie d'importance entre les différentes clés: \texttt{lname} est la clé centrale et contient presque toujours des informations, \texttt{fname} des données secondaires et \texttt{date} des dates. Les autres clés sont rarement utilisées si l'entrée de catalogue ne correspond pas à une personne.

\begin{listing}
	\begin{minted}{xml}
<item n="271" xml:id="CAT_000327_e271">
	<!-- ... -->
	<name type="author">Turenne (Henri de La Tour d'Auvergne vicomte de)</name>
	<trait>
		<p>illustre maréchal de France, né en 1611, tué en 1675.</p>
	</trait>
	<!-- ... -->
</item>
	\end{minted}
	\caption{L'entrée \xmltei{} à partir de laquelle des données sont extraites}
	\label{code:prepin}
\end{listing}

\begin{listing}
	\begin{minted}{python}
{
	"fname": "henri ", 
	"lname": "la tour d'auvergne", 
	"nobname_sts": "Turenne ", 
	"status": "", 
	"dates": "1611 1675 ", 
	"function": "marshal", 
	"rebuilt": False
}
	\end{minted}
	\caption{La sortie \json{} correspondante}
	\label{code:prepout}
\end{listing}

\subsubsection{Présentation de l'algorithme d'extraction d'informations}
L'algorithme détaillé ci dessous est présenté sous forme graphique dans la figure \ref{fig:extractmain}. Cette étape peut être séparée en deux parties différentes: l'extraction d'informations nominatives du \tname{} et la récupération de données biographiques du \ttrait{}.

L'extraction d'informations du \tname{} est l'étape plus complexe. La difficulté tient au fait que cette balise peut contenir des informations variées et structurées de façon très différente. Cela demande d'identifier des motifs récurrents et de les repérer dans le texte à différents degrés et à différentes étapes. Dans un premier temps, les prénoms sont systématiquement extraits. Ils sont toujours détectés -- même si la balise ne contient le nom d'une personne: cette extraction permet justement d'identifier le type de données contenues dans le \tname{}. Les prénoms sont repérés à l'aide de plusieurs \glspl{expression régulière} qui permettent d'identifier des prénoms complets et abrégés, qu'ils soient composés ou non. Cette détection prend en compte les différents types d'abréviations possibles (un prénom composé peut être entièrement abrégé; à l'inverse, seulement un des prénoms peut être abrégé) et les différentes typographies (séparer les prénoms avec des traits d'union ou non, par exemple). Dans le cas où un prénom serait abrégé, il est si possible reconstruit: des initiales sont remplacées par un nom complet; ce processus est présenté plus en détail ci-dessous. Cela permet d'augmenter le taux de réussite dans l'alignement avec des identifiants \wkd{}, mais vient avec plusieurs difficultés techniques, comme nous le verrons. Une fois ce nom extrait, le type d'information contenue dans le \tname{} doit être identifié: en fonction du type d'information (géographique, historique, nominative...), différents traitements sont mis en place. Cette identification se fait par détection de motifs augmentée par l'usage de \glspl{table de conversion}\footnote{Pour un exemple de table de conversion, voir \ref{appendix:convfunction}} et de listes contenant du vocabulaire spécifique. Listes et tables étant classées thématiquement, il est possible, par un processus éliminatoire, d'identifier avec certitude le type d'information contenue dans le \tname{}. Le traitement du \tname{} dépend grandement de cette détection: si cette balise contient des éléments géographiques ou historiques, l'extraction d'informations repose en grande partie sur les tables de conversion. S'il s'agit en revanche d'un nom de personne, il est alors nécessaire d'identifier les différents types de données nominatives (prénom, nom de famille, nom de famille noble...) pour bien structurer les données. En effet, c'est de cette structure que dépend la bonne construction du \gls{dictionnaire}, et donc la constitution de requêtes adaptées à l'\api{}. Ce processus d'extraction des données s'appuie majoritairement sur une détection de motifs. Le motif déterminant est la présence ou non dans parenthèses dans le nom. Comme nous le verrons, si un nom contient des parenthèses, les informations sont bien plus structurées que s'il n'en contient pas. Les informations peuvent alors être extraites avec une bien plus grande granularité.

Une fois les informations nominatives extraites du \tname{}, il reste à extraire les données biographiques pertinentes du \ttrait{}. Cette étape, plus simple que la précédente, vaut principalement pour les entrées où c'est l'auteur.ice d'un document qui est mentionné.e dans le \tname{}. Les seules informations extraites concernent les dates de vie et de mort des personnes, ainsi que son métier. Quelques difficultés techniques subsistent cependant: il faut notamment distinguer une date de naissance/décès d'une autre date, afin de diminuer le bruit; de plus, il faut réussir à extraire de façon automatique l'occupation principale d'une personne, lorsque plusieurs personnes sont mentionnées (\enquote{militaire} et \enquote{auteur}, par exemple). La résolution de ces deux problèmes repose sur une bonne connaissance du corpus de catalogues et de la structure des \ttrait{}.



\widepage
\begin{figure}[!p]
	\centering
	\tikz[node distance=1cm, scale=0.70, transform shape]{
		\node[db]%
		(input) at (-12,2)%
		{Données en entrée};
		\node[base]%
		(start) at (-3,2)% 
		{Lecture des données et lancement de l'algorithme};
		\node[base]%
		(tname) at (-3,0)%
		{Étape 1 -- Extraction de données du \tname{}};
		\node[transf]%
		(rebuild) at (7,-4)%
		{Extraction du prénom et reconstruction d'un prénom complet à partir de sa version abrégée (\ref{fig:abv2full})};
		\node[choice]%
		(type) at (-3,-4)%
		{Détection du type de \tname{}};
		\draw[arrow] (input) -- (start);
		\draw[arrow] (start) -- (tname);
		\draw[arrow] (tname) -- (type);
		\draw[arrow] (tname) -- (7,0) -- (rebuild);
		
		\node[base]%
		(meme) at (-12, -8)%
		{Même personne que l'entrée précédente};
		\node[transf]%
		(memetr) at (-12, -11)%
		{Réutilisation du dictionnaire de l'entrée précédente; le script passe à l'entrée suivante};
		\draw[arrow] (type) -- (meme);
		\draw[arrow] (meme) -- (memetr);
		
		\node[base]%
		(div) at (-7,-8)%
		{Pas d'informations (ex: \enquote{Divers})};
		\node[transf]
		(divtr) at (-7,-11)%
		{Pas d'extraction d'informations};
		\draw[arrow] (type) -- (div);
		\draw[arrow] (div) -- (divtr);
		
		\node[base]%
		(pers) at (-2,-8)%
		{\tname{} contenant le nom d'une personne};
		\node[choice]%
		(persch) at (-2,-14)%
		{Détection du type de nom: avec/sans parenthèses};
		\node[transf]%
		(nopartr) at (-5, -19)%
		{Nom sans parenthèses: nettoyage simple des données \\ Réinjection du prénom extrait au début dans le \texttt{fname} et du reste dans le \texttt{lname}};	
		\node[transf]%
		(partr) at (1, -19)%
		{Nom avec parenthèses: voir \ref{fig:extractparenthesis} \\ Injection du prénom extrait en entrée du script};
		\draw[arrow] (type) -- (pers);
		\draw[arrow] (pers) -- (persch);
		\draw[arrow] (persch) -- (nopartr);
		\draw[arrow] (persch) -- (partr);
		
		\node[base]%
		(geo) at (3,-8)%
		{\tname{} avec informations géographiques};
		\node[transf]%
		(geotr) at (3,-11)%
		{Extraction d'informations à l'aide de tables de correspondance et ajout au \texttt{lname}};
		\draw[arrow] (type) -- (geo);
		\draw[arrow] (geo) -- (geotr);
		
		\node[base]%
		(hist) at (8,-8)%
		{\tname{} historique};
		\node[transf]%
		(histtr) at (8,-11)%
		{Extraction d'informations à l'aide de tables de correspondance et ajout au \texttt{lname}};
		\draw[arrow] (type) -- (hist);
		\draw[arrow] (hist) -- (histtr);
		
		\node[base]%
		(ttrait) at (-2,-23)%
		{Étape 2 -- Extraction de données du \ttrait{}};
		\node[transf]%
		(date) at (-5,-25)%
		{Extraction des dates de vie et de mort et ajout au \texttt{date}};
		\node[transf]%
		(func) at (1,-25)%
		{Extraction et traduction du métier et ajout au \texttt{function}};
		
		\node[base]%
		(end) at (-2,-27)%
		{Dictionnaire structuré pour lancer les requêtes};
		
		\begin{pgfonlayer}{bg} % \draw on the background of nodes
			\draw[dotted] (rebuild) -- (7,-6.5) -- (0.75,-6.5) -- (partr);
			\draw[dotted] (rebuild) -- (7,-6.5) -- (-4.5,-6.5) -- (nopartr);
			\draw[arrow] (divtr) -- (-7,-23) -- (ttrait);
			\draw[arrow] (nopartr) -- (-5,-23) -- (ttrait);
			\draw[arrow] (partr) -- (1,-23) -- (ttrait);
			\draw[arrow] (geotr) -- (3,-23) -- (ttrait);
			\draw[arrow] (histtr) -- (8,-23) -- (ttrait);
			\draw[arrow] (ttrait) -- (date);
			\draw[arrow] (ttrait) -- (func);
			\draw[arrow] (date) -- (-5,-27) -- (end);
			\draw[arrow] (func) -- (1,-27) -- (end);
			\draw[arrow] (memetr) -- (-12,-27) -- (end);
		\end{pgfonlayer}
	}
	\caption{Processus d'extraction d'informations du \tname{} et \titem{}}
	\label{fig:extractmain}
\end{figure}
\restoregeometry

\subsection{Identifier le type de nom}
Les éléments \texttt{tei:name} contiennent le titre donné à l'item vendu. Si c'est souvent le nom de l'auteur.ice du document, ce n'est pas toujours le cas. L'extraction d'éléments du \tname{} dépend, comme cela a été dit, de l'identification du \enquote{type} de nom. La décision a été prise de classer tous les \tname{} en cinq catégories.

Les noms génériques: ces éléments ne contiennent pas d'informations précises. Les entités \wkd{} étant spécifiques plutôt que génériques, il n'est pas certain que les entrées puissent être alignées avec \wkd{}; si des entités \wkd{} correspondent  à ces éléments, les informations qu'elles contiennent seront probablement trop génériques pour être utilisables dans un contexte économétrique. Lorsque cela est possible, l'alignement est quand même fait; c'est le cas par exemple pour les \tname{} contenant la mention de chartes. Dans ces cas, un dictionnaire vide est retourné et l'alignement avec \wkd{} n'aura pas lieu. Une exception est faite dans le cas chartes, où un alignement est fait avec l'entité \wkd{} génériques. Dans cette catégorie se trouve par exemple: 
\begin{itemize}
	\item \mintinline{xml}|<name type="author">DIVERS</name>|
\end{itemize}

Les noms de type \enquote{Le même} ou \enquote{La même}; lorsqu'il y a cette valeur dans le \tname{}, l'auteur.ice est la même personne que l'auteur.ice de l'entrée de catalogue précédente. Dans ce cas, le dictionnaire de cette entrée est réutilisé. Par exemple:
\begin{enumerate}
	\item \mintinline{xml}|<name type="author">Le même</name>|.
\end{enumerate}

Les noms géographiques; ces entrées sont détectées à l'aide de tables de conversion et de listes. Celles-ci sont classées thématiquement: liste d'anciennes colonies françaises (\ref{appendix:convcolonie}), de départements français du \scl{XIX}(\ref{appendix:convdpt}), d'anciennes provinces françaises (\ref{appendix:convprov}) et de pays (\ref{appendix:convcountry}). Dans ce cas, un alignement avec une entité \wkd{} est possible; cependant, il n'est pas toujours envisageable de retrouver l'entité précise. En effet, un \tname{} peut contenir une mention d'une donnée géographique, mais également d'autres détails. C'est par exemple le cas dans le quatrième exemple ci-dessous. Il faut alors aligner le \tname{} avec son équivalent générique sur \wkd{} (l'exemple ci-dessous, par exemple, a été aligné uniquement avec l'entité \enquote{Paris}). Dans cette catégorie se trouvent:
\begin{itemize}
	\item \mintinline{xml}|<name type="other">AISNE (département de 1')</name>|
	\item \mintinline{xml}|<name type="author">Bourbonnais. </name>|
	\item \mintinline{xml}|<name type="author">Paris : Musée royal du Louvre</name>|
	\item \mintinline{xml}|<name type="author">Garde nationale parisienne en 1792 (brevet|
\end{itemize}
\begin{itemize}[label={}]
	\item \mintinline{xml}|de volontaire de la)</name>|
\end{itemize}

Les noms correspondant à des évènements historiques. Là encore, une table de conversion est utilisée (\ref{appendix:convevt}). Ici, une difficulté apparaît cependant: du fait de la varité des évènements historiques mentionnés dans les entrées de catalogue, il n'est pas possible d'enregistrer l'ensemble des évènements dans des tables afin de permettre une détection de tous les évènements. Les \tname{} ont donc été analysés pour extraire les évènements les plus importants. Ensuite, comme pour les termes géographiques, il n'est pas possible de donner aux tables de conversion une granularité suffisamment fine pour contenir toutes les données possibles. Des alignements partiels ont donc été faits: le premier exemple ci-dessous a été aligné avec l'entité \wkd{} \enquote{Révolution française}.
\begin{itemize}
	\item \mintinline{xml}|<name xmlns="http://www.tei-c.org/ns/1.0" type="other">THÉATRE|
\end{itemize}
\begin{itemize}[label={}]
	\item \mintinline{xml}|RÉVOLUTIONNAIRE</name>|
\end{itemize}
\begin{itemize}
	\item \mintinline{xml}|<name type="author">COMMUNE DE 1871.</name>|
	\item \mintinline{xml}|<name type="other">Siège de La Rochelle en 1628</name>|
\end{itemize}

Les noms de personnes. Ceux-ci ne sont pas simples à traiter: ils peuvent contenir de nombreuses informations: deux noms de famille (usuels et nobles), titres de noblesse, plusieurs prénoms. Ils ont également une structure variée, comme cela apparaît dans les exemples ci-dessous: les noms peuvent être écrits en utilisant des parenthèses ou non; un prénom peut être écrit en entier, comme dans premier exemple, entièrement abrégé (comme dans l'exemple 3) ou encore partiellement abrégé, ce qui est le cas dans le troisième exemple. Ces différences, qui ne posent pas de problème à un regard humain, sont autant de problèmes techniques. En effet, la détection de motifs fonctionne uniquement sur des critères formels, ou structurels. Il faut donc, en s'appuyant sur la structure des documents, réussir à distinguer un prénom d'un nom de famille, un nom de famille d'un autre, ou encore un nom abrégé d'un nom complet.
\begin{itemize}
	\item \mintinline{xml}|<name type="author">Humboldt (le baron Alexandre de)</name>|
	\item \mintinline{xml}|<name type="author">Taccani Tasca (madame la comtesse)</name>|
	\item \mintinline{xml}|<name type="author">LEGOUVÉ (G. M. J. B.)</name>|
	\item \mintinline{xml}|<name type="author">LOUIS XVIII</name>|
	\item \mintinline{xml}|<name type="author">Duras (Emm.-F. de Durfort, duc de)</name>|
\end{itemize}

Si l'on pose l'extraction d'informations nominatives sur une personne comme étant l'objectif principal, une difficulté apparaît vite: qu'est-ce qui distingue un nom de personne d'un des autres types de noms? Les éléments \tname{} sont voués à contenir des noms propres; chercher à distinguer les noms propres des noms communs n'a donc pas d'intérêt. Cela est d'autant plus que la graphie varie d'un catalogue à l'autre: dans certains, les majuscules sont signifiantes et pourraient permettre de distinguer noms communs et noms propres, tandis que dans d'autres, l'intégralité du \tname{} est en majuscule. Une détection de motifs à partir de simples critères formels (du type: \enquote{Un nom de personne est un ou plusieurs mots commençant par des majuscules}) n'est pas opérante pour ce corpus. Il n'est pas non plus possible de définir un nom positivement, puisqu'il n'existe aucun critère définitoire pour un nom propre. Enfin, à cette étape, il n'est pas non plus possible de s'appuyer sur l'extraction de prénoms. L'extraction de prénoms se base, comme on le verra, sur de la détection de motifs; là encore, ce qui est identifié comme un prénom peut tout aussi bien être un nom propre, et il n'existe à ce stade aucune possibilité pour distinguer un nom propre d'un autre. C'est à ce stade qu'a été décidée l'utilisation de tables de conversion et de listes contenant du vocabulaire thématique: en identifiants des références récurrentes à des évènements ou des lieux dans les \tname{}, il devient possible de définir les noms de personne négativement. En définitive, un nom de personne, c'est donc ce qui n'est pas autre chose et la détection du type de nom fonctionne donc de manière éliminatoire.

L'algorithme mène donc une série de tests, cherchant à détecter si un \tname{} rentre dans telle ou telle catégorie. Du fait du fonctionnement technique de \py{}, une série de tests éliminatoires doit aller du cas le plus particulier au cas le plus générique afin d'éviter les faux positifs: une fois qu'un élément a été détecté comme appartenant à une catégorie, il ne peut plus être réassigné à une autre. C'est pourquoi l'algorithme commence par chercher à classer les éléments dans les catégories où le taux d'erreur est le plus faible, pour ensuite finir par la catégorie la plus générique: celle des noms de personne.: c'est dans ces catégories que le taux d'erreur est le plus faible. 

Le script commence donc par chercher à classer un \tname{} dans les catégories où les informations sont écrites plus ou moins toujours de la même manière. Il cherche d'abord à identifier si le \tname{} correspond à \enquote{Le même} ou \enquote{La même}. Dans ce cas, le \tname{} est le même que celui de l'entrée précédente et c'est ce \gls{dictionnaire} qui est réutilisé. Ensuite, les entrées génériques sont détectées. Comme elles contiennent toujours des informations écrites de la même manière (\enquote{Documents divers}), le taux d'erreur est là encore très faible. Ces entrées génériques ne contiennent pas d'informations spécifiques; si un \tname{} appartient à cette catégorie, alors l'algorithme n'extrait pas d'informations. 

Ensuite, si un \tname{} n'appartient ni à l'une ni à l'autre catégorie, alors l'algorithme cherche à classifier un nom en différentes catégories à l'aide de tables de comparaison et de listes contenant du vocabulaire spécifiques. Ces tables et listes sont classées en deux catégories (données historiques et géographiques) afin de définir des traitements spécifiques; l'algorithme cherche d'abord à identifier des entrées géographiques, puis historiques. En effet, un bien plus grand nombre de tables contenant des données géographiques existe\footnote{Nom d'anciennes provinces françaises (\ref{appendix:convprov}), de départements du \scl{XIX} (\ref{appendix:convdpt}), d'anciennes colonies (\ref{appendix:convcolonie}) et de pays (\ref{appendix:convcountry})}, ce qui augmente les possibilités d'un classement correct. Enfin, le script cherche à identifier des informations historiques dans un \tname{} (\ref{appendix:convevt}). Si une donnée géographique ou historique est repérée, alors équivalents normalisés de cette donnée sont ajoutés au \gls{dictionnaire} grâce à l'usage de tables de conversions. 

Par processus d'élimination, si aucune de ces informations n'a été détectée, alors il n'est plus possible de classer le \tname{} dans aucune autre catégorie. Il est alors considéré que le contenu du \tname{} est le nom d'une personne. L'extraction d'informations se fait ici plus complexe \footnote{Pour une représentation graphique de cette étape, voir \ref{fig:extractparenthesis}}, comme nous le verrons: le script traite le nom différemment en fonction de sa structure (présence ou non de parenthèses dans le nom), puis extrait d'éventuels titres de noblesse, noms de famille noble et nom de famille usuel. Enfin, il extrait des prénoms et cherche à reconstruire un prénom complet à partir de son abréviation. Le processus étant éliminatoire, il n'y a bien sûr aucune certitude totale que le contenu du \tname{} soit bel et bien le nom d'une personne; il n'est cependant plus possible de mieux catégoriser les éléments. Cela n'est pas non plus extrêmement important, puisque cet algorithme de classification a un rôle fonctionnel et n'impacte pas la manière dont les identifiants sont récupérés depuis l'\api{} \wkd{}. Il permet principalement d'adopter un fonctionnement modulaire en cherchant à détecter des motifs spécifiques dans les \tname{} en fonction de la catégorie à laquelle ils appartiennent. La détection de motifs étant \enquote{aveugle}, le traitement qui est fait pour cette catégorie peut être mis à profit de différents types de données: les mêmes motifs peuvent être recherchés et extraits de différents types d'entrées.

À l'issue de cette phase de classification, il est possible de mieux connaître les types de \tname{} et leur répartition dans le corpus. Les noms de personnes sont très majoritaires dans les \tname{}: ils représentent 80634 entrées sur un total de 82913, soit 97,25\% du corpus. C'est donc pour ce type de données que l'extraction des données a été pensée, ce qui permet  d'extraire des informations avec une granularité plus fine qu'avec le reste du corpus. Viennent ensuite les noms de lieux (1406 entrées) et les éléments divers (550). Pour finir se trouvent les évènements historiques (232 entrées) et les éléments vides, soit 92 entrées\footnote{Ces chiffres proviennent d'un calcul réalisé à partir du script développé pour identifier le type d'entrée. Ils ont été produits afin de réaliser une représentation graphique du type d'entrée, visible en annexes (\ref{appendix:tnametypes})}. Bien que, comme cela a été dit, la classification des \tname{} en différents types soit purement fonctionnelle, ces chiffres permettent d'avoir une meilleure connaissance du corpus, et mieux comprendre à partir de quel type de données se fait l'alignement \wkd{}. Cette classification montre également d'intérêt de la détection de motifs augmentée de tables de correspondances: bien que ce soit une méthode \enquote{légère}, elle peut efficacement, sur un corpus semi-structuré, être à la base d'une extraction précise de données.


\subsection{Le traitement des noms de personnes}
Comme cela a déjà été dit, le processus d'extraction d'informations du \tname{} est relativement simple pour les entrées qui ne contiennent pas de noms de personnes: il s'agit principalement de remplacer des informations non-normalisées par leur équivalent normalisé, à l'aide d'un dictionnaire. Pour les noms de personne cependant, le processus est plus complexe: il demande d'identifier titres de noblesse et différents types de noms; de plus, de nombreux prénoms sont abrégés pour gagner de la place dans les catalogues. Pour augmenter le taux de réussite de l'alignement avec \wkd{}, une méthode a été développée pour reconstruire un nom complet à partir de son abréviation. Après avoir présenté les différentes étapes de l'extraction d'informations nominatives, cette partie détaillera ce processus de reconstruction du prénom. Dans ces deux étapes, la nature semi-structurée des documents est centrale, puisqu'elle permet d'identifier avec une grande précision les différentes informations.

\subsubsection{Trouver des solutions adaptées à différents types de noms}
Pour extraire des informations d'un \tname{} et dans leur un statut ou d'une signification spécifique, il est nécessaire d'identifier ce qui distingue un prénom d'un nom de famille ou d'un titre de noblesse. Une première difficulté apparaît vite: formellement, il n'y a pas nécessairement de différence entre ces informations. Dans l'exemple \ref{code:prepin}, le \tname{} correspond à:

\begin{listing}[h!]
	\centering
	\begin{minted}{xml}
<name type="author">Turenne (Henri de la Tour d'Auvergne vicomte de)</name>
	\end{minted}
	\caption{Le \tname{} de l'exemple \ref{code:prepin}}
	\label{code:prepin_teiname}
\end{listing}

Les informations à extraire de cet élément sont visibles dans la figure \ref{code:prepin_teiname_part}. Si chaque mot est pris individuellement, il est difficile de les distinguer l'un de l'autre. Le nom de famille noble, le prénom et le nom de famille usuels débutent tous par une majuscule. Il est ici impossible de trouver un motif distinctif pour un nom de famille ou pour un prénom. Cela est d'autant plus vrai qu'un nom peut être composé; ici, le nom de famille usuel est composé de deux mots débutant par une majuscule et séparés par un \enquote{d'}. Étant donné la taille du corpus, il n'est pas non plus possible d'utiliser des tables de conversions pour identifier ce qui est un nom ou un prénom, comme cela a été fait pour les lieux et les évènements géographiques. La seule information qui peut être traitée à l'aide d'une table de conversion est le titre de noblesse, puisque ces titres existent en nombre limité. Par conséquent, aucun élément à l'intérieur des termes à relever ne permet de les identifier. Là où ils peuvent cependant être distingués, c'est au niveau de leur place relative au sein de la phrase. Pour reprendre l'exemple \ref{code:prepin_teiname_part}, le nom de famille noble se retrouve hors de la parenthèse, au tout début de la phrase; les autres informations sont dans la parenthèse, où le prénom est suivi du nom de famille et du titre. S'il n'est pas possible d'identifier la valeur d'un nom à partir de ses caractéristiques propres, il est donc possible de la déduire de sa position dans la phrase. C'est ici qu'une bonne compréhension de la structure des \tname{} devient essentielle.

\begin{figure}[h!]
	\centering
	\tikz{
		\node[baseoutline,%
			draw=nadeshikopink,%
			label={below:nom de famille noble}]%
			at (-10.5,0)%
			{Turenne};
		\node[baseoutline]%
			at (-8.5,0)%
			{(};
		\node[baseoutline,%
			draw=purple,%
			label={below:prénom}]%
			at (-7.25,0)%
			{Henri};
		\node[baseoutline]%
			at (-5.5,0)%
			{de la};
		\node[baseoutline,%
			draw=red,%
			label={below:nom de famille usuel}]%
			at (-2.5,0)%
			{Tour d'Auvergne};
		\node[baseoutline,%
			draw=orange,%
			label={below:titre de noblesse}]%
			at (1.3,0)%
			{vicomte};
		\node[baseoutline]%
			at (3,0)%
			{de)};
	}
	\caption{Les différentes parties du \tname}
	\label{code:prepin_teiname_part}
\end{figure}

Il est donc nécessaire d'identifier les différentes structures possibles pour un \tname{} contenant un nom de personne. Celles-ci sont visibles en \ref{fig:teinametypes}. Pour les besoins de l'algorithme, deux structures principales ont été retenues: les \tname{} contenant des parenthèses et eux n'en contenant pas\footnote{Comme cela étant déjà visible dans la figure \ref{fig:extractmain}}. Comme nous le verrons, d'autres subdivisions existent ensuite.

Les \tname{} contenant des parenthèses ont une structure bien plus claire que ceux n'en contenant pas; il est alors tout à fait possible d'identifier le rôle joué par les différents éléments de cette balise. L'organisation entre les différents types de noms est la même dans les exemples \ref{code:teiname_noble} et \ref{code:teiname_notnoble}, qui contiennent tous les deux des parenthèses. Le nom le plus important, sous lequel une personne est connue au \scl{XIX}, est contenu entre parenthèses et au début de l'entrée, parfois en majuscules. Le ou les noms suivants sont, eux, à l'intérieur des parenthèses. Le premier nom au sein des parenthèses est toujours le prénom; ensuite viennent des informations complémentaires. Ces deux exemples, cependant, forment deux sous-catégories dans le groupe des noms sans parenthèses:
\begin{itemize}
	\item Le premier exemple (\ref{code:teiname_noble}) contient uniquement des noms de personnes nobles. Dans ce cas, le nom hors parenthèses correspond au nom de famille noble; le nom de famille usuel d'une personne est contenu à l'intérieur des parenthèses. Ce détail est important, parce que les personnes sont souvent référencées sur \wkd{} par le nom de famille usuel. Lancer une recherche sur l'API avec les noms de famille usuel et noble à la fois ne retournera pas toujours de réponse, alors que ne rechercher qu'un des deux noms peut permettre d'obtenir le bon résultat. C'est pourquoi, dans ces cas, les deux noms sont distingués: le nom de famille usuel est associé avec la clé \texttt{lname} du \gls{dictionnaire}, et le nom noble lié au \texttt{nobname\_sts}.
	\item Les personnes dans le second exemple (\ref{code:teiname_notnoble}) ne sont pas nobles, ce qui simplifie beaucoup l'extraction d'information. Il suffit de stocker la partie entre parenthèses hors du \texttt{lname}; le prénom entre parenthèses, identifié grâce à l'algorithme d'extraction des prénoms, est stocké dans le \texttt{fname}. Parfois, d'autres informations sont contenues dans les parenthèses; si elles sont signifiantes (comme dans le cas d'Alexandre Dumas père, où il est important de faire la distinction d'avec son fils), elles sont extraites. Sinon, ces informations supplémentaires sont abandonnées.
\end{itemize}

\begin{listing}[h]
	\begin{minted}{xml}
<name type="author">Turenne (Henri de la Tour d'Auvergne vicomte de)</name>
<name type="author">HUMBOLDT (Alexandre baron de)</name>
<name type="author">Tascher (Pierre-Jean-Alexandre-Jacquemin comte Imbert de)<name>
	\end{minted}
	\caption{Trois exemples de \tname{} avec titres de noblesse}
	\label{code:teiname_noble}
\end{listing}

\begin{listing}[h]
	\begin{minted}{xml}
<name type=author>Viardot (Pauline)</name>
<name type=author>Verdi (Giuseppe)</name>
<name type=author>Sobieski (Thérèse-Cunégonde)</name>
<name type="author">CAUCHY (le bon Alex.)</name>
<name type="author">DUMAS (Alex. père)</name>
	\end{minted}
	\caption{Cinq exemples de \tname{} sans titres de noblesse}
	\label{code:teiname_notnoble}
\end{listing}

Les \tname{} ne contenant pas de parenthèses (exemple \ref{code:teiname_nopar}) ne peuvent être traités avec la même granularité, puisqu'ils ne présentent pas de caractères récurrents pour faire la différence entre leurs différentes parties. Il est donc impossible de s'appuyer de façon récurrente sur la position des différents noms dans la phrase pour déduire leur signification. Cependant, il faut également remarquer que les éléments ne contenant pas de parenthèses contiennent en général bien moins d'informations que ceux qui en contiennent. Chercher à tout prix à identifier une structure à ces entrées afin de pouvoir avoir un \gls{dictionnaire} complet à partir duquel lancer plusieurs requêtes n'a pas non plus nécessairement d'intérêt.

\begin{listing}[h]
	\begin{minted}{xml}
<name type="author">Sophie</name>
<name type="author">Henri VI</name>
<name type="author">DUPUIS</name>
	\end{minted}
	\caption{Trois exemples de \tname{} sans parenthèses}
	\label{code:teiname_nopar}
\end{listing}

\begin{figure}[p]
	\centering
	\tikz{
		\node[base] (start) at (0,3)%
		{Analyse de la structure d'un \tname{}};
		\node[choice] (choicepar) at (0,0) %
		{Présence ou non de parenthèse};
		\node[base] (yespar) at (-6,-2.5)%
			{
				\textbf{Présence d'une parenthèse}
				\\~\\ \textit{Structure de l'entrée}:
				\begin{itemize}
					\item nom le plus important au début hors parenthèses
					\item noms moins importants dans la parenthèse
				\end{itemize}
			};
		\node[base] (nopar) at (6.5,-3.5) %
			{
				\textbf{Absence de parenthèse}
				\\~\\ \textit{Structure de l'entrée}: moins claire et régulière.
				\\~\\ \textit{Informations à récupérer}:
				\begin{itemize}
					\item titre de noblesse
					\item noms et prénoms ensemble, sans distinction
				\end{itemize}
			};
				
		\node[choice] (choicenoble) at (-2.5,-7) %
			{Présence ou non d'un titre de noblesse};
		
		\node[blank] at (-4,-9) {NON};
		\node[blank] at (-1.1,-8.5) {OUI};
		
		\node[base] (ynoble) at (1.5,-15) %
			{
				\textbf{Présence d'un titre de noblesse}. 
				\\~\\ \textit{Structure de l'entrée}:
				\begin{itemize}
					\item nom de famille noble hors parenthèses
					\item entre parenthèses: prénoms, puis, nom de famille usuel et enfin titre de noblesse
				\end{itemize}
				\textit{Informations à récupérer}:
				\begin{itemize}
					\item prénom
					\item nom de famille usuel
					\item nom de famille noble
					\item titre de noblesse
				\end{itemize}
			};
		\node[base] (nnoble) at (-5.5,-15)%
			{
				\textbf{Absence de titre de noblesse}.
				\\~\\ \textit{Structure de l'entrée}:
				\begin{itemize}
					\item nom de famille usuel
					\item prénom dans les parenthèses, suivi éventuellement d'autres informations
				\end{itemize}
				\textit{Informations à récupérer}:
				\begin{itemize}
					\item prénom
					\item nom de famille usuel
				\end{itemize}
			};
		
		\draw[arrow] (start) -- (choicepar);
		\draw[arrow] (choicepar) -- (yespar);
		\draw[arrow] (choicepar) -- (nopar);
		\draw[arrow] (yespar) -- (choicenoble);
		\draw[arrow] (choicenoble) -- (nnoble);
		\draw[arrow] (choicenoble) -- (ynoble);	
	}
	\caption{Différentes structures possibles pour un \tname{}}
	\label{fig:teinametypes}
\end{figure}


\subsubsection{Identifier et extraire les informations nominatives}
C'est sur les différents critères présentés dans la partie précédente et résumés sous forme graphique dans la figure \ref{fig:teinametypes}, que se base l'extraction d'informations nominatives du \tname{}.


Dans un premier temps, l'algorithme cherche à extraire des informations d'un \tname{} contenant des parenthèses (étape représentée dans le graphique: \ref{fig:extractparenthesis}). Il y a alors quatre données à identifier et à extraire: le nom de famille noble, le nom de famille usuel, le prénom et le titre de noblesse. L'algorithme commence par extraire des informations du texte contenu entre parenthèses avant de passer au texte hors parenthèses: il identifie un titre de noblesse avant d'extraire un prénom et éventuellement un nom de famille; pour finir, le nom hors parenthèses est extrait.

L'extraction du titre de noblesse est l'étape la plus simple, puisqu'elle repose sur une table de conversion (\ref{appendix:convstatus}): l'algorithme cherche à identifier un titre de noblesse parmi les clés de ce \gls{dictionnaire}. Si des titres sont trouvés, alors leur version normalisée figurant en valeur de ce dictionnaire est ajoutée aux données extraites. Il est à noter que, comme tous les titres de noblesse les plus importants permettent d'obtenir des résultats sur \wkd{}, aucune valeur n'est associée aux titres les moins importants; dans ce cas, aucun titre n'est extrait, puisqu'il risque d'empêcher l'obtention d'un résultat. Cependant, même des titres de noblesse n'ayant pas d'équivalent, et qui ne seront donc pas extraits, figurent sur cette table de conversion: la table permet d'identifier la présence d'un titre de noblesse, ce qui est nécessaire pour extraire les autres informations.

Si un titre de noblesse a été trouvé, un processus éliminatoire est engagé afin d'identifier le prénom et le nom de famille usuels contenus entre parenthèses. Comme cela apparaît dans les exemples plus haut, le texte entre parenthèses peut contenir du bruit: des mots comme \enquote{du, le...} qui n'aident pas à obtenir de résultats sur \wkd{}, ou encore des noms communs et des attributs des personnes. Il faut détecter tous les noms propres, situer leur rôle et les extraire. Il peut être tentant d'extraire seulement les noms propres à l'aide d'\glspl{expression régulière} -- c'est à dire, d'identifier tous les mots commençant par une majuscule -- et d'abandonner le reste. Cependant, certains noms communs peuvent être écrits avec une capitale; en suivant cette méthode, il y aurait donc un risque d'inclure du bruit dans les données utilisées pour lancer les requêtes. L'algorithme lance donc une série de simplifications du contenu entre parenthèses: il commence d'abord par supprimer les données déjà extraites: le prénom ou son abréviation identifiées et le titre de noblesse; ensuite, il supprime l'ensemble des termes \enquote{auxilliaires} (\enquote{le, la, dit...}), qu'ils aient des capitales ou non; enfin, tous les noms communs restants sont supprimés. Il ne reste alors du texte entre parenthèses qu'un ou plusieurs noms commençant par une majuscule et considérés comme des noms propres; quelle valeur donner à ce texte? Étant donné que le prénom a déjà été identifié, de même que le titre de noblesse, le texte entre parenthèses ne peut être qu'un nom de famille; on considère alors que le texte entre parenthèses est un de nom de famille usuel; le texte hors parenthèses est alors le nom de famille noble.

Si un \tname{} contient des parenthèses mais pas de titre de noblesse, le processus est analogue à celui décrit ci-dessus -- mais plus simple, puisque le texte entre parenthèses a tendance à être bien plus complexe pour les personnes nobles. Le bruit est successivement détecté et supprimé à l'aide d'\glspl{expression régulière}; le prénom déjà extrait est supprimé. À ce stade, une deuxième extraction de prénoms a lieu: les prénoms étant parfois complexes et écrits d'une manière variable (avec ou sans parenthèses...), il arrive que ceux-ci ne soient pas entièrement extraits. Cette seconde extraction pose une légère difficulté, puisque l'on dispose alors de deux noms propres ayant la valeur d'un prénom. Il faut déterminer la relation entre ces deux \enquote{prénoms}: les mettre simplement bout-à-bout risquerait de créer des noms et prénoms dans le désordre; dans ces cas, le moteur de recherche de \wkd{} ne retournerait pas de résultat. Par conséquent, c'est en fonction de la position relative des deux \enquote{prénoms} que le prénom final est reconstruit: celui qui se situe au début du texte entre parenthèse est positionné en premier dans le prénom reconstruit. Pour finir, c'est le texte hors parenthèses qui est extrait, soit le nom de famille. En l'absence de titre de noblesse, il n'est pas nécessaire de faire la différence entre différents noms de famille; le nom hors parenthèses ne peut être qu'un nom de famille usuel.

Si un nom ne contient pas de parenthèses, le processus d'extraction est à la fois plus simple et moins qualitatif: il est tout simplement impossible de classer les différentes données extraites par détection de motifs. Dans ce cas là, le bruit supprimé du \tname{}; l'objectif est de pouvoir lancer des recherches sur \wkd{} avec les données les plus propres possibles, afin de maximiser les possibilités d'obtenir un résultat correct.

\widepage
\begin{figure}[!p]
	\centering
	\tikz[node distance=1cm]{
		\node[base](start) at (0,0)%
		{\tname{} contenant des parenthèses};
		\node[base] (fname) at (8,0)%
		{Prénoms extraits du \tname{} et éventuellement reconstruits (\ref{fig:abv2full})};
		\node[label={[rotate=-90]below:Réinjection des prénoms déjà extraits}] at (8.75,-4.25) {};
		
		\node[choice](nobl) at (0,-3)%
		{Est-ce que le \tname{} contient un titre de noblesse?};
		\node[label={OUI}] at (-2.25,-5.25) {};
		\node[label={NON}] at (2.25, -5.25) {};
		
		\node[transf](y1) at (-4,-8)%
		{Extraction des données entre parenthèses: titre de noblesse (ajouté au \texttt{status}) et nom de famille personnel ajouté au \texttt{lname}};
		\node[transf](y2) at (-4,-13)%
		{Injection des prénoms extraits à l'épape précédente dans le \texttt{fname}};
		\node[transf](y3) at (-4,-17.5)%
		{Extraction des données en dehors des parenthèses: le nom de famille noble, ajouté au \texttt{nobname\_sts}};
		
		\node[transf](n1) at (4,-8.5)%
		{Extraction des données entre parenthèses: seconde extraction de prénoms pour réduire le bruit et nettoyage};
		\node[transf](n2) at (4,-13.5)%
		{Réinjection des prénoms extraits et combinaison des prénoms extraits pour constituer le \texttt{fname}};
		\node[transf](n3) at (4,-17.5)%
		{Extraction du nom de famille hors parenthèses et constitution du \texttt{lname}};
		
		\node[base](end) at (0,-21)%
		{Passage à l'extraction d'informations du \texttt{lname}};
		
		\begin{pgfonlayer}{bg} % \draw on the background of nodes
			\draw[arrow] (start) -- (nobl);
			\draw[arrow] (nobl) -- (y1);
			\draw[arrow] (y1) -- (y2);
			\draw[arrow] (y2) -- (y3);
			\draw[arrow] (nobl) -- (n1);
			\draw[arrow] (n1) -- (n2);
			\draw[arrow] (n2) -- (n3);
			\draw[arrow] (n3) -- (4,-21) -- (end);
			\draw[arrow] (y3) -- (-4,-21) -- (end);
			\draw[dotted,] (fname) -- (8,-13.5) -- (n2);
			\draw[dotted] (fname) -- (8,-11) -- (-3.5,-11) -- (y2);
		\end{pgfonlayer}	
	}
	\caption{Extraction d'informations d'un \tname{} contenant des parenthèses}
	\label{fig:extractparenthesis}
\end{figure}
\restoregeometry


\subsubsection{Reconstruire un prénom complet à partir de son abréviation}
Cette phase essentielle vise à corriger un problème dans les données: le nom d'une personne est souvent abrégé. Cela pose un problème pour l'utilisation ultérieure de l'\api{}: le prénom est une information essentielle pour identifier l'alignement avec la bonne entité sur \wkd{}. Sans ce prénom, le risque est grand que le premier résultat obtenu à l'aide du moteur de recherche de \wkd{} ne soit pas celui de la personne décrite dans le \tname{}, mais celui d'un.e autre membre de la même famille. De plus, si une requête est lancée avec un prénom abrégé (les initiales d'une personne, par exemple), elle ne retourne pas de résultat\footnote{Par exemple, voici une recherche sur Jean-Jacques Ampère. Si l'on recherche \enquote{J J Ampère}, les \href{https://www.wikidata.org/w/index.php?search=j+j+amp\%C3\%A8re}{résultats obtenus sont incorrects}. En recherchant \enquote{Jean Jacques Ampère}, le \href{https://www.wikidata.org/w/index.php?search=jean+jacques+amp\%C3\%A8re}{résultat obtenu est le bon}.}. Le choix a donc été fait, pour chaque entrée, d'essayer de reconstruire un prénom complet à partir de son abréviation. Comme cela a été montré plus haut, l'extraction du prénom est également essentielle pour identifier les autres informations du \tname{} par élimination. La reconstruction des noms a lieu en trois étapes: 
\begin{itemize}
	\item D'abord, un prénom, abrégé ou complet, est extrait.
	\item Ensuite, le type de prénom est identifié. Pour cet algorithme, trois types de prénoms existent: les prénoms complets (composés ou non), les prénoms abrégés non composés et les prénoms abrégés composés. Le choix de faire une distinction entre les deux catégories s'explique par la différence dans le processus de reconstruction
	\item Enfin, l'algorithme cherche à reconstruire le prénom à l'aide de tables de conversions.
\end{itemize}

La première question qui se pose est: qu'est-ce qui constitue un prénom? Celui-ci correspond à un motif (un ou plusieurs noms propres), mais il est identifié par sa position dans la phase: le prénom est la première information contenue entre parenthèses -- cela vaut autant pour les noms de personnes nobles que pour les autres. Pour être certain de cibler uniquement le prénom, cette étape n'a donc lieu que dans les \tname{} contenant des parenthèses. Bien qu'un motif \enquote{prénom} puisse être identifié et que la position du prénom soit fixe, il est en fait assez difficile de modéliser ce motif de façon précise. Il faut définir un ou plusieurs motifs qui correspondent à tous les cas de figures, à toutes les formes que peuvent prendre un prénom. Voici quelques exemples pour mieux cibler cette difficulté (les prénoms sont en gras):
\begin{itemize}
	\item \enquote{Sobieski (\textbf{Thérèse-Cunégonde})}: un cas classique: prénom composé où les deux noms sont complets et séparés par des tirets.
	\item \enquote{Zimmermann (\textbf{P.-J.-G.})}: une autre forme simple: un nom composé où seules restent les initiales, écrites en majuscules, terminées par des points et séparées par des tirets.
	\item \enquote{DUBOIS-FONTANELLE (\textbf{J. Gaspard})}: une forme un peu plus complexe: le nom est partiellement abrégé, avec une initiale et un nom complet; cependant, il n'y a pas de tiret qui sépare les deux prénoms, et il faut donc que les deux parties soient détectées toutes les deux dans le même prénom (sans quoi, il existe un risque de se retrouver avec des prénoms partiels).
	\item \enquote{DESCHAMPS (\textbf{Jh Fr. L.})}: pour finir, une forme qui mélange les complexités: trois prénoms abrégés, mais qui ne contiennent pas qu'une initiale; il ne sont pas tous terminés par des points et ne sont pas séparés par des tirets.
\end{itemize}

Cette variété tient autant du bruit dans les données que de différentes formes de notation dans le corpus. Elle complexifie en tout cas la définition, d'un point de vue formel, de ce qui constitue un prénom; il est notamment difficile d'extraire l'intégralité d'un prénom composé, puisqu'il faut faire en sorte que les différents noms ou initiales soient reconnus comme faisant partie du même nom, qu'ils soient séparés par des tirets ou non. Dans l'abstrait.
\begin{itemize}
	\item Un nom composé abrégé est une série de majuscules suivies d'autres lettres et/ou de points; il doit contenir au moins un point et un tiret (un motif plus précis risque de ne pas identifier tous les cas de figure; un motif plus généraliste risque d'inclure trop de bruit).
	\item Un nom simple abrégé correspond à une majuscule, suivie ou non d'autres lettres et terminée par un point.
	\item Un nom complet est une série de majuscules suivies de minuscules, séparées ou non par des tirets.
\end{itemize}


Identifier ces motifs \enquote{dans l'abstrait} est une étape importante, qui permet de mettre en place une implémentation technique. Cependant, il est impossible de construire un motif unique correspondant à tous les cas de figures. La décision a donc été d'accepter cette variété et ce bruit et de créer une série de motifs sous la forme d'\glspl{expression régulière}, qui permettent de cibler différents cas de figure.\footnote{Les trois fonctions permettant l'extraction de prénoms sont visibles ici; \ref{appendix:rgxabvcomp} permet d'identifier les noms composés abrégés, \ref{appendix:rgxabvsimp} identifie des noms abrégés non composés et \ref{appendix:rgxfull} se consacre aux noms complets}. À partir d'une connaissance du corpus et de sa structure, il est donc possible d'identifier tous les prénoms dans le texte, et d'automatiser cette identification à l'aide de langages de programmation détectant des motifs.

Une fois les prénoms identifiés, il s'agit de reconstruire ceux qui sont abrégés. Comme pour le reste du processus d'extraction d'informations, cette étape s'appuie fortement sur l'usage de tables de conversion. Le principe général est simple: des tables de conversion sous la forme de \glspl{dictionnaire} contiennent en clés des formes abrégées de noms, et en valeurs les formes complètes. Si un nom extrait se retrouve en clé de ces tables, alors il est remplacé par sa version complète. Malheureusement, l'implémentation technique n'est pas aussi facile. En regardant les tables de conversions utilisées (\ref{appendix:namecomp}, \ref{appendix:namesimp}), il apparaît que celles-ci sont assez courtes, et ne contiennent pas une grande variété de noms. Le problème avec l'usage de tables de conversion, c'est d'abord qu'elles doivent être faites à la main, et qu'il n'est pas possible de prendre toutes les abréviations possibles en compte; mais le véritable problème est qu'une seule abréviation peut faire référence à plusieurs noms. C'est malheureusement le cas des abréviations les plus banales: \enquote{J.} peut à la fois signifier \enquote{Jean}, \enquote{Jeanne}, \enquote{Joséphique}...  Le choix a donc été fait de ne retenir que les abréviations \enquote{évidentes} pour éviter d'introduire du bruit supplémentaire. De plus, quand un nom est modifié par l'algorithme de reconstruction, cela est indiqué à l'aide de la clé \texttt{rebuilt} du dictionnaire utilisé pour lancer des requêtes (\ref{code:prepout}).

L'algorithme de reconstruction des prénoms a été conçu pour chercher à palier à cette relative pauvreté des données de conversion, surtout dans le cas des noms composés abrégés. Ceux-ci posent en effet un problème au delà de leur simple extraction: presque toutes les combinaisons de prénoms sont possibles. Pour plus de clarté, ce processus sera décrit à partir de l'exemple de \enquote{J.-P.-Ch.}, à partir duquel il faut arriver à \enquote{Jean-Pierre-Charles}. La reconstruction d'un prénom composé commence par une tentative d'alignement complet avec les données de la table de conversion: un remplacement ne sera fait que si le prénom abrégé correspond exactement à une clé de la table de conversion contenant des noms composés (\ref{appendix:namecomp}). Si il y a alignement complet, alors l'extraction du prénom s'arrête ici. Dans notre exemple, ce n'est pas le cas. Il faut tout de même tenter de reconstruire un prénom, ne serait-ce que partiel. Pour minimiser le risque d'erreur, cette reconstruction partielle se fait en deux temps. Dans un premier temps, l'algorithme cherche à associer une partie des initiales extraites avec des clés de la table consacrée aux noms composés. Dans l'exemple, \enquote{J.-P.} serait transformé en \enquote{Jean-Pierre}, puisque \enquote{j p} se trouve dans la table de conversion. Si l'intégralité du nom abrégé avait été reconstruit, l'algorithme s'arrêterait là. Dans notre exemple, il reste à transformer le \enquote{Ch.} en \enquote{Charles}. L'algorithme essaye donc ensuite de l'aligner les initiales restantes avec des clés de la table consacrée aux noms non composés (\ref{appendix:namesimp}). \enquote{ch} étant présent dans cette table, le dernier nom abrégé est remplacé par un nom complet: un nom a donc été entièrement reconstruit à partir de son abréviation. Tout au long de ce processus, un système permet de suivre quelles initiales ont été remplacées et lesquelles restent à être traitées, pour éviter de traduire deux fois une seule initiale. À la fin de ce processus, toutes les initiales qui n'ont pas été remplacées sont supprimées: un nom composé partiel (qui ne contient pas tous les noms) peut permettre d'obtenir des résultats sur l'\api{} \wkd{}; cependant, un nom contenant encore des initiales risque de renvoyer du silence.


Souvent, le prénom d'une personne est écrit en abrégé. Partant de ce constat, un algorithme a été construit pour:
\begin{itemize}
	\item Repérer lorsqu'un prénom est abrégé, en prenant en compte différents types d'abréviations (nom simple ou composé, nom entièrement ou partiellement abrégé) et des possibles fautes dans les catalogues (un point est oublié à la fin d'une abréviation, par exemple).
	\item Reconstruire un prénom complet à partir de son abréviation, ce qui passe pas un algorithme qui cherche à reconstruire le nom en plusieurs étapes pour obtenir le nom le plus complet possible avec un mimimum d'erreurs.
\end{itemize}

Ici, l'approche est totalement prédictive: il est impossible d'être certain d'obtenir le bon nom complet à partir de son abréviation; on peut uniquement prédire que le prénom reconstruit sera conforme au vrai prénom (tel qu'il est écrit sur \wkd{}) et chercher à maximiser cette certitude.

\begin{landscape}
\begin{figure}[p]
	\centering
	\tikz[scale=0.70, transform shape]{
		\node[base] (start) at (0,3)%
			{Contenu du \tname{} entre parenthèses};
		\node[transf] (extr) at (0,0)%
			{Extraction de prénom complet ou abrégé par détection de motif};
		\node[choice] (type) at (0,-4)%
			{Quel type de nom a été détecté?};
		\node[base] (abvcomp) at (-13,-7)%
			{Prénom abrégé composé};
		\node[base] (abvsimp) at (0,-7)%
			{Prénom abrégé non composé};
		\node[base] (full) at (10,-7)%
			{Prénom complet non abrégé};
			
		\node[transf] (compfullmatch) at (-13,-11)%
			{Tentative d'alignement de l'intégralité du prénom avec un prénom composé non abrégé};
		\node[transf] (comppartmatch) at (-6.5,-11)%
			{Tentative d'alignement d'une partie du prénom avec un prénom composé non abrégé};
		\node[transf] (simpmatch) at (0,-11)%
			{Tentative d'alignement d'un prénom abrégé non composé avec un prénom complet};
		\node[choice] (fullmatch) at (11,-11)%
			{S'agit-il véritablement d'un nom complet ou est-ce un faux positif?};
		\node[transf] (fullfake) at (6,-16)%
			{C'est une abréviation \\ Extraction d'un nom complet simple ou composé};
		\node[transf] (fulltrue) at (13,-16)%
			{Ce n'est pas une abbréviation \\ Pas de modifications du nom};
			
		\node[blank] at (-9.6,-10.5) {ECHEC*};
		\node[blank] at (-3.1,-10.5) {ECHEC*};
		\node[blank] at (-11.7,-14) {REUSSITE};
		\node[blank] at (-5.2,-14) {REUSSITE};
		\node[blank] at (6.7,-13.4) {ABRÉVIATION};
		\node[blank] at (14,-14) {NOM COMPLET};
		\node[base, fill=white] at (13,0) {*: L'échec peut être total (aucun prénom n'a été extrait) \\ ou partiel (une partie du prénom seulement a été extraite).};
				
		\node[base] (end) at (0,-20)%
			{Prénom extrait et recomposé si possible};
			
		\draw[arrow] (start) -- (extr);
		\draw[arrow] (extr) -- (type);
		\draw[arrow] (type) -- (abvcomp);
		\draw[arrow] (type) -- (abvsimp);
		\draw[arrow] (type) -- (full);
		\draw[arrow] (abvsimp) -- (simpmatch);
		\draw[arrow] (abvcomp) -- (compfullmatch);
		\draw[arrow] (compfullmatch) -- (comppartmatch);
		\draw[arrow] (comppartmatch) -- (simpmatch);
		\draw[arrow] (full) -- (fullmatch);
		\draw[arrow] (fullmatch) -- (fullfake);
		\draw[arrow] (fullmatch) -- (fulltrue);
		\draw[arrow] (compfullmatch) -- (-13,-20) -- (end);
		\draw[arrow] (comppartmatch) -- (-6.5,-20) -- (end);
		\draw[arrow] (simpmatch) -- (end);
		\draw[arrow] (fullfake) -- (6,-20) -- (end);
		\draw[arrow] (fulltrue) -- (13,-20) -- (end);
	}
	\caption{Représentation graphique du processus d'extraction et de reconstitution d'un nom à partir de son abréviation}
	\label{fig:abv2full}
\end{figure}
\end{landscape}

\subsection{Extraire des informations normalisées à partir d'un nom et de sa description}
Cette sous-section détaille l'utilisation de tables de conversion pour traduire et normaliser certaines donnes importantes (dates de vie et mort, titres de noblesse et fonctions).

BIEN PENSER à parler du problème des informations extraites, mais qui ne se rattachent pas à la personne du \tname{} (eg: femme du comte...) => comment, par détection de motifs, est-ce que l'on règle le problème.

\section{Extraire des identifiants \wkd{}}
Une fois un dictionnaire de données normalisées produites, un algorithme lance des recherches en plein texte sur l'\api{} de \wkd{} afin de récupérer des identifiants. L'algorithme lance plusieurs requêtes successivement. L'objectif est de récupérer un identifiant en lançant le moins de requêtes, avec le plus de certitude possible.

\subsection{Présentation générale}
Ici est présenté le fonctionnement général de l'algorithme, qui se comporte différemment en fonction du type de données qu'il a à traiter (personne noble ou non, prénom reconstruit ou non...).

\subsection{Gérer la montée en charge: optimisation et réduction du temps d'exécution}
Le script est assez compliqué, repose sur une \api{} et traite un grand nombre de données (plus de 82000 entrées). Il prend donc plus d'une dizaine d'heures à s'exécuter et demande des ressources élevées (la première version du script ne fonctionnait plus sur mon ordinateur après avoir traité 5\% du jeu de données). L'optimisation nécessaire de l'algorithme est décrite dans cette sous-section.

\subsection{Évaluation du script : tests, performance et qualité des données extraites de \wkd{}}
Des tests ont été réalisés pour:
\begin{itemize}
	\item isoler l'impact de chaque paramètre (élément du dictionnaire) dans l'obtention des bons résultats
	\item évaluer la qualité de l'algorithme final
	\item mesurer la performance de celui-ci.
\end{itemize}
Ces tests, et leurs résultats, sont présentés ici.

\section{Après l'alignement, l'enrichissement: utiliser \sparql{} pour produire des données structurées}
La récupération des identifiants \wkd{} est la partie la plus complexe dans l'utilisation de \wkd{} pour enrichir des données. Après une présentation des informations requêtées via \sparql{}, le processus d'extraction d'informations et de stockage dans un \json{} est détaillé.

\subsection{Quelles données rechercher via \sparql?}
Après avoir expliqué pourquoi développer des méthodes d'enrichissement automatique, une seconde question se pose: quelles sont les données à récupérer? Cette question n'est pas anodine du fait de la diversité du corpus. Le titre donné à un manuscrit dans un catalogue et inscrit dans le \tname{} est souvent celui d'une personne, mais ce n'est pas toujours le cas. Il arrive également qu'un manuscrit soit nommé d'après un évènement (la Révolution française), un lieu ou une province (Italie, Poitou), ou encore une typologie de document (des chartes).

Un bref retour sur la manière dont fonctionne \sparql{} permet de mieux comprendre le problème que peut poser la diversité du corpus. \sparql{} a l'avantage de permettre de récupérer des données propres sur des bases de données en ligne; cependant, l'information y est organisée de manière très spécifique, ce qui demande de faire des requêtes précises. Ce langage de requêtes a pour but d'interagir avec une base de données au format \rdf{}. Avec ce format -- dit sémantique -- les données sont organisées en \enquote{triplets} sujet--prédicat--objet, où:

\begin{itemize}
	\item le sujet est la ressource principale.
	\item le prédicat est une propriété du sujet, qui caractérise une relation avec une autre ressource, l'objet.
	\item l'objet est une ressource secondaire: c'est la valeur d'un prédicat.
\end{itemize}

Le principe des triplets \rdf{} est mieux exprimé sous forme graphique (\ref{fig:triplet}):

\begin{figure}[!h]
	\centering
	\tikz{
		\node%
		[base] %
		(S) at (0,0) %
		{\textbf{Sujet} \small \\ La ressource principale \\ \textit{Natalia Gontcharova}};
		\node%
		[transf] %
		(P) at (5, 0) %
		{\textbf{Prédicat} \small \\ La relation à l'objet \\ \textit{a peint}};
		\node%
		[base] %
		(O) at (10, 0) %
		{\textbf{Objet} \small \\ Une ressource secondaire \\ \enquote{\textit{Les porteuses}}};
		
		\draw[arrow] (S) -- (P);
		\draw[arrow] (P) -- (O);
	}
	\caption{Exemple de relation sujet -- prédicat -- object}
	\label{fig:triplet}
\end{figure}

Deux particularités supplémentaires définissent les formats sémantiques:
\begin{itemize}
	\item Toutes les \enquote{ressources} peuvent être tour à tour sujet ou objet. L'exemple du dessus, par exemple, aurait pu être réécrit sous la forme: \enquote{Les porteuses} a été peint par Natalia Gontcharova. Dans ce cas, Natalia Gontcharova est l'objet et \textit{Les porteuses} est le sujet. Par conséquent, une base de données \rdf{} est une base de donnée en graphes; elle peut être représentée sous la forme d'un réseau de ressources qui entretiennent des relations bilatérales entre elles. Il n'y a pas de hiérarchie entre les informations, contrairement à une base de données \xml{} classique.
	\item L'ensemble des ressources et des prédicats d'une base de donnée en graphe sont définis et disposent d'un identifiant unique. Les prédicats, plus particulièrement, sont définis selon une ontologie particulière.
\end{itemize}

Le deuxième point complexifie la définition des données à récupérer via \sparql{}: les prédicats sont décrits avec une grande précision; par conséquent, une information analogue peut être représentée par différents prédicats dans différentes situations. Dans l'ontologie \wkd{}, la création d'un texte et la création d'une peinture ne correspondent pas au même prédicat. Pour que les données soient utilisables, il faut être très spécifique quant aux informations recherchées. 

Les 18899 entités avec lesquelles les entrées de manuscrits ont été alignées peuvent se classer en de nombreuses catégories. Sur \wkd{}, une entité est une \enquote{instance} d'une classe plus large. En suivant la classification de \wkd{}, les entités appartiennent aux catégories suivantes\footnote{Un graphique présentant l'ensemble de ces catégories se trouve en annexes (\ref{appendix:wikidata_instances}).}:
\begin{itemize}
	\item personnes humaines; cette catégorie est la plus fréquente (12090 occurrences)
	\item noms de familles (3180 entités)
	\item communes françaises (586 occurrences)
	\item peintures et sculptures (respectivement 520 et 236 entités)
\end{itemize}

Cette variété peut s'expliquer en partie par le taux d'erreur dans l'alignement avec Wikidata. Cependant, toutes ces \enquote{erreurs} ne correspondent pas forcément à des résultats qui ne sont pas pertinents. Par exemple, l'algorithme peut aligner un.e écrivain.e avec un de ses ouvrages, ou une personne avec son portrait. Des résultats erronés peuvent toujours garder une forme de pertinence. Il est d'autant plus important de construire des requêtes \sparql{} qui se concentrent pas uniquement sur des personnes. Cependant, il n'est pas possible de faire une requête qui correspondent à toutes les catégories auxquelles appartiennent le corpus. Le choix a donc été fait de se concentrer sur les catégories les plus pertinentes: les personnes, les familles, et les œuvres artistiques et littéraires. Non seulement ces catégories contiennent la grande majorité du corpus (16026 entités), mais ces catégories sont les plus à même de contenir des entités pertinentes. Il a été choisi de ne pas faire de requête spécifique sur les lieux, puisque peu d'informations sont disponibles pour les entités de la catégorie \enquote{communes françaises} sur Wikidata.

Un nombre assez conséquent de données ont donc été requêtées avec \sparql{}, du fait des spécificités des bases de données en graphes, de la variété des entités \wkd{} auxquelles les manuscrits sont liées, et enfin du fait de la variété du corpus lui même. Ces informations récupérées correspondent aux différentes catégories de \wkd{}.
\begin{itemize}
	\item Pour les personnes et les familles, les informations suivantes sont récupérées sur \wkd{}:
	\begin{itemize}
		\item Le genre de la personne;
		\item Sa nationalité, afin de voir si l'origine d'une personne influence le prix d'un manuscrit;
		\item Les langues parlées par une personne; là encore, l'objectif est d'étudier l'impact de l'origine d'un.e auteur.ice sur un prix.
		\item Les date de vie et de mort, afin de placer un manuscrit dans une époque et de voir comment son ancienneté et sa contemporanéité en influencent le prix.
		\item Le lieu où une personne est née, où elle a vécu et où elle est morte, pour des raisons analogues.
		\item La manière dont la personne est morte. Si cette information peut sembler anecdotique à un public contemporain, les catalogues de ventes sont marqués par un goût du sensationnel, et la manière dont une personne est morte est souvent mentionnée, notamment en cas d'exécutions.
		\item La religion d'une personne: il peut être intéressant d'étudier si, et comment, ce critère influence l'évolution d'un prix.
		\item Les titres de noblesse d'une personne.
		\item L'éducation qu'a reçu une personne, afin de mieux situer ses occupations et d'analyser l'impact du niveau et du type d'éducation sur le prix.
		\item L'occupation d'une personne, et les fonctions précises qu'elle a occupées: là encore, il est intéressant de situer l'impact de la carrière sur le prix et de voir quelles occupations sont corrélées avec des prix élevés sur le marché des manuscrits.
		\item Les prix et distinctions reçus par une personne. À l'aide de ce critère, il est alors possible de chercher à répondre à cette question: la célébrité d'une personne de son vivant impacte-elle le prix de se manuscrits?
		\item Les organisations et institutions dont la personne est membre (Académie française, Franc-maçonnerie...)
		\item Le nombre d'œuvres écrites ou réalisées par une personne. Là encore, c'est une tentative de mesurer l'impact ou la célébrité d'une personne: les manuscrits de quelqu'un ayant beaucoup écrit sont ils plus chers que les manuscrits d'une personne ayant peu écrit ?
		\item Le nombre de conflits auxquels une personne a participé. Ce critère de recherche permet de quantifier l'importance d'un personnage militaire.
		\item Des images, telles que le portrait et la signature.
	\end{itemize}
	\item Pour les créations littéraires, ce sont des informations bibliographiques qui sont avant tout récupérées; pour les autres œuvres d'art, des informations analogues sur le contexte de création sont retenues.
	\begin{itemize}
		\item Le titre de l'œuvre.
		\item Son auteur.ice, pour étudier si certain.e.s auteur.ice.s sont susceptibles d'influencer le prix d'un manuscrit.
		\item La date de création de l'œuvre, afin de savoir si l'époque d'origine influence le prix. Pour les livres, la date publication est également récupérée.
		\item La requête récupère aussi la maison d'édition d'un livre.
		\item Les dimensions et matériaux d'une œuvre d'art sont également d'intérêt.
		\item Enfin, le genre et le mouvement dans lequel s'inscrivent une œuvre sont d'intérêt: ces informations pourraient permettre de voir si une hiérarchie des du goût influence le prix d'un manuscrit.
	\end{itemize}
	\item Pour finir, afin de pouvoir éventuellement enrichir nos données avec d'autres sources externes à \wkd{}, des identifiants uniques ont été récupérés afin de donner accès à d'autres bases de données en ligne: les identifiants VIAF (Fichier d'autorité international virtuel), ISNI (International Standard Name Identifier), de la Bibliothèque nationale de France, de la Bibliothèque du Congrès américain, ainsi que les identifiants IDRef. Certaines insitutions, comme la BnF, rendent leurs données accessibles via \sparql{}; la récupération de ces identifiants faciliterait grandement les enrichissements ultérieurs depuis d'autres sources de données.
\end{itemize}

Comme on l'a dit, l'objectif principal de l'alignement avec \wkd{} est de produire des données pour calculer des régressions linéaires, ce qui permettrait d'étudier les déterminants du prix d'un manuscrit sur le marché du \scl{XIX}. Cette récupération d'informations en masse ouvre d'autres possibilités. Entre autres, de nombreuses données géographiques ont sont récupérées (lieu de naissance, de mort, d'enterrement). Il est ensuite possible  de récupérer les coordonnées de ces lieux, afin de construire une cartographie des auteur.ice de manuscrits circulant sur le marché parisien du \scl{XIX} parisien. Cette possibilité n'est pas anodine, puisqu'elle permettrait de mettre en relation la \enquote{parisianité} avec la construction du canon littéraire à Paris. Il serait également possible d'étudier la circulation des productions culturelles, et leur rayon d'influence. En croisant les données géoréférencées avec des données chronologiques (dates de naissance et de mort...), ces questions peuvent également être étudiées de façon historique: comment l'influence de l'origine géographique sur la réception d'une œuvre évolue au fil des siècles? Répondre à ces questions n'a pas été possible dans le cadre de mon stage; cependant, grâce à l'enrichissement de données via \sparql{}, il de telles études deviennent possibles, et les données pour mener ces analyses sont au moins en partie déjà disponibles. Produire des informations normalisées et exploitables pour la recherche implique donc de produire des données réutilisables, qui doivent être réutilisables avec d'autres problématiques de recherches.

\subsection{Présentation générale}
Comme pour les autres étapes, on présente ici, à l'aide d'un schéma, la structure générale de l'algorithme de requêtes.

\subsection{Développer un comportement uniforme pour produire des données exploitables à partir un corpus hétérogène}
Ici est détaillée 
\begin{itemize}
	\item la requête \sparql{} lancée (subdivisée en plusieurs petites requêtes).
	\item le format de sortie produit à partir des données renvoyées par \sparql
\end{itemize}

\subsection{Minimiser la perte: optimisation et gestion des erreurs}
Comme des quantités massives de requêtes sont lancées, et que de très nombreuses informations sont demandées, des erreurs peuvent avoir lieu, et notamment des erreurs de \textit{timeout} (le temps d'exécution dépasse la durée autorisée). La gestion de ces erreurs est décrite ici.

\subsection{Lier la \tei{} aux données nouvellement produites}
Cette courte section détaille la mise à jour des fichiers \tei{} avec les identifiants \wkd{}, ce qui permet de faire le lien entre les entrées de catalogues et les données issues de \wkd{}.

\section{Des données à la monnaie: premiers résultats de l'étude}
Sous réserve que l'étude des régressions linéaires ait été fait à temps (ce qui n'est pas garanti), j'aimerais ici présentés les premiers résultats sur les facteurs de l'évolution des prix.