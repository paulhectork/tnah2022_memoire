%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%CONCLUSION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\backmatter
\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

% à propos du matching complet fait en 2
Ainsi, en s'appuyant sur une connaissance du corpus, il est possible d'approximer une véritable compréhension \enquote{humaine} d'un texte semi structuré, en utilisant uniquement des \glspl{expression régulière}. Si il n'est pas possible d'identifier le sens des différents éléments, il est possible de localiser et d'extraire les éléments signifiants. Il apparaît alors que ces éléments signifiants sont, d'une certaine manière, des formes, ou des motifs qui peuvent être détectés automatiquement; la signification peut être identifiée en fonction du type de donnée extraite (un nombre est une date). Mais, dans un corpus semi-structuré, il surtout est possible d'inférer du \enquote{sens} d'un élément à partir de sa position dans le texte. C'est dans ce cadre qu'un encodage en \xmltei{} des documents prend tout son intérêt: les différentes parties d'un texte sont balisées sémantiquement, ce qui est essentiel au traitement automatisé du texte: il devient possible de traiter le texte non pas dans son intégralité, mais au niveau d'une unité sémantique très précise, comme le nom donné à un manuscrit (le \tname{}) ou une brève description (le \ttrait{})

Le fait de travailler sur des corpus semi-structurés permet également de mettre en approche des méthodes alternatives, \enquote{low-tech}, pour le traitement du langage et l'analyse textuelle dans des projets de recherche. Avec une bonne connaissance du corpus et des données structurées, il est possible de se baser entièrement sur des solutions techniquement \enquote{simples}, telles que la détection de motifs à l'aide d'\glspl{expression régulière}. Plutôt que de s'appuyer sur de l'apprentissage machine, et donc des solutions complexes, il est intéressant de s'appuyer sur des méthodes alternatives et de développer des algorithme plus complexes, basés sur une approche modulaire, qui sont capables de cibler très précisément les éléments d'un texte pertinent pour des problématiques de recherche précises. Ces méthodes de basse technologie, qui s'intéressent à un usage intelligent des technologies plutôt qu'à des technologies \enquote{intelligentes} forment un contre modèle qui gagnerait à être mis en avant dans les projets de recherche. L'intelligence artificielle devenue un outil central et pertinent dans les humanités numériques -- à la base de la reconnaissance optique de caractères. Mais elle ne doit pas seulement être une nouveauté que les projets de recherche doivent à tout prix incorporer. C'est également une technologie extrêmement polluante\footnote{
	Strubell, Ganesh et McCallum (\cite{strubell_energy_2019}) ont étudié l'impact écologique des modèles d'apprentissage profond en traitement du langage naturel. En étudiant les ressources nécessaires à l'entraînement d'un modèle, les auteur.ice.s montrent (\cite[p. 1]{strubell_energy_2019}) que la mise au point d'un modèle \textit{Transformer} peut émettre jusqu'à 626155 livres de CO\textsubscript{2}, soit cinq fois la consommation d'une voiture durant toute sa vie (126000 livres de CO\textsubscript{2}) et 17 fois les émissions de CO\textsubscript{2} émises par une personne américaine en un an (36156 livres). Il.elle.s rappellent également que les modèles d'apprentissage machine les plus performants sont aussi les plus gourmands en ressources et qu'un modèle doit être entraîné parfois plusieurs milliers de fois pour atteindre des scores satisfaisants (\cite[p. 1]{strubell_energy_2019}). Avec l'augmentation générale des performances des modèles de traitement du langage naturel, l'augmentation marginale de leur performance est de plus en plus coûteuse en ressources: l'augmentation du \gls{bleu} d'un modèle de traduction automatique de 0.1 points de performance (pour atteindre un score de 29,7) a coûté 150000 dollars en ressources environnementales et en coût de calcul (\cite[p. 4]{strubell_energy_2019}).
}, utilisée de façon massive par des grands groupes industriels\footnote{
	\textit{Google}, et sa filiale \textit{Youtube}, s'appuient depuis le milieu des années 2010 de plus en plus massivement sur l'intelligence artificielle (\cite[p. 1]{covington_deep_2016}). L'algorithme de suggestion de vidéos sur \textit{Youtube} repose entièrement sur de l'apprentissage machine(\textit{ibid.}): en fonction de l'historique de l'utilisateur.ice, l'algorithme \enquote{apprend} à suggérer du contenu pertinent. L'usage massif de telles technologies est particulièrement polluant: pour pouvoir suggérer des vidéos pertinentes, il faut, à chaque connexion de l'utilisateur.ice, consulter deux très grandes bases de données: dans un premier temps, l'algorithme sélectionne toutes les vidéos potentiellement pertinentes, depuis l'ensemble des vidéos disponibles sur le site; ensuite, ces vidéos sont classées en consultant une seconde base de données et en filtrant les résultats obtenus par le comportement récent de l'utilisateur.ice (ce qu'il ou elle a recherché, combien de temps il ou elle a passé à regarder une vidéo, sa réaction à celle-ci...). Le 2 décembre 2020, \textit{Google} a \href{https://googlewalkout.medium.com/setting-the-record-straight-isupporttimnit-believeblackwomen-5d7bbfe4ed90}{licencié la docteure Timnit Gebru}, alors à la tête de l'équipe spécialisée en éthique de l'intelligence artificielle, suite à la rédaction d'un article sur les impacts écologiques de l'utilisation massive par l'entreprise de modèles de traitement automatisé du langage. Ce phénomène alarmant rappelle au passage la non-neutralité des publications scientifiques provenant ce cette entreprise, dont deux ont été citées dans le présent mémoire (\cite{mikolov_efficient_2013, covington_deep_2016}).
} et un argument de vente pour de nombreuses start-up, alors que les applications pratiques de l'intelligence artificielle soulèvent de graves problèmes éthiques\footnote{À ce sujet, voir par exemple le projet \href{https://github.com/daviddao/awful-ai}{\textit{Awful AI} de David Dao}, qui liste une quantité impressionnante d'utilisations préoccupantes de telles technologies.}. La recherche en humanités numériques, fondée sur une connaissance des techniques et des documents utilisés, devrait fournir un espace pour remettre en question de telles technologies et pour développer des solutions alternatives, où la connaissance scientifique supplante l'intelligence machine. De façon modeste, le projet \mssktb{} offre une telle remise en question: en s'appuyant sur une bonne connaissance de corpus textuels semi-structurés, le projet a développé une chaîne de traitement entièrement basée sur la détection de motifs. S'appuyer sur des méthodes de basse technologie ne vaut bien sûr pas seulement comme une déclaration de bonne foi: plus une solution est techniquement simple et accessible, plus elle est réutilisable, adaptable à d'autres jeux de données et plus facilement elle peut évoluer pour incorporer de nouvelles fonctionnalités.


pollution des large language models:
https://arxiv.org/abs/1906.02243

----- parler du minimal computing ??
