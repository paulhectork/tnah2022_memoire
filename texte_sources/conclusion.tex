%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%£%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% CONCLUSION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%£%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
\markboth{\textsc{CONCLUSION}}{\textsc{CONCLUSION}}
Ce mémoire se veut une présentation des possibilités offertes par l'utilisation de documents catalographiques dans un projet de recherche en humanités numériques. En tant que sources historiques homogènes et diachroniques, les catalogues sont des documents qui se prêtent bien à l'utilisation de méthodes computationnelles. En effet, celles-ci ont l'avantage sur des formes d'analyses traditionnelles de permettre un traitement en masse et quasi-instantané de données. Cependant, un ordinateur étant incapable de lire, ou d'approximer une compréhension humaine d'un document, la complexité des sources devient très vite un obstacle pour leur manipulation automatisée. C'est là que réside l'intérêt des catalogues. Ces documents contiennent par définition de grandes quantités d'information sous forme synthétique, structurée et répétitive. Ils offrent donc une simplicité et une régularité qui rend possible leur manipulation.

La modélisation des données constitue une bonne première manière d'aborder des catalogues à l'aide de méthodes informatiques. En effet, l'encodage est une étape qui rend nécessaire une réflexion sur les documents sources, et qui permet de faire la transition entre le papier et le numérique. La modélisation nécessaire du texte permet de mieux comprendre comment celui-ci s'organise, ce qui est déjà une première étape de l'analyse des données. Toute modélisation, tout encodage \enquote{reflète une théorie du texte}, comme le dit Sperberg-McQueen, l'un des créateur.ice.s du standard \xml{} et l'un des instigateur.ice.s de la \tei{}\footnote{\enquote{\textit{Markup reflects a theory of text}}, Sperberg-McQueen dans \cite{renear_refining_1996}}. Encoder un document revient donc à faire des choix sur ce qu'est un document, et ces choix déterminent ce qui sera possible ou non avec des méthodes computationnelles. C'est donc une étape essentielle du traitement automatisé d'un corpus de texte. Quelle est, alors, la théorie des catalogues exprimée par le projet \mssktb{}? Ce que l'encodage dit des données du projet, c'est que celles-ci sont prises dans une optique pragmatique, comme des informations à traiter et manipuler. En effet, les choix d'encodage mettent l'accent sur une structure uniforme pour un corpus de catalogues qui est en fait très hétérogène; de plus, ce n'est pas le catalogue comme publication qui est reflété dans les éditions \xmltei{}, mais le catalogue comme source de données: les seules pages encodées sont celles qui contiennent des items en vente, et donc des descriptions de manuscrits. En mettant en avant la régularité de ces documents semi-structurés, les catalogues encodés fonctionnent mieux comme une base de données prête à l'emploi que comme une édition scientifique exhaustive. L'encodage numérique permet également d'expliciter, lorsque cela est possible, la nature semi-structurée des catalogues encodées. L'encodage touche cependant ici à ses limites: les catalogues contenant du texte en langage naturel, il n'est pas possible d'encoder les éléments dans une entrée de catalogue qui sont répétitifs, mais sans être pour autant toujours les mêmes. Il reste une part du catalogue qui échappe à la rigueur de l'encodage, et c'est cette part qui est caractéristique de la complexité des sources textuelles qui sont l'objet des humanités numériques. C'est pour faire face à cette complexité propre aux données textuelles que s'est développée une chaîne de traitement complète d'enrichissement et d'extraction d'informations des catalogues.

Là où l'encodage touche  à ses limites, le traitement automatisé du texte prend la relève. En effet, celui-ci offre plus de souplesse qu'un encodage, et peut s'adapter à différents cas de figures afin de traiter le corpus de catalogues dans toute sa complexité. Avec des méthodes computationnelles, il est possible de repérer les différents motifs qui se répètent d'une entrée à l'autre, et de s'appuyer sur ces conventions propres à la langue du corpus à l'étude pour développer une chaîne de traitement. Ainsi, en s'appuyant sur une connaissance du corpus, il est possible d'approximer une véritable compréhension \enquote{humaine} d'un texte semi structuré, en utilisant uniquement des \glspl{expression régulière}. Si il n'est pas possible d'identifier le sens des différents éléments, il est envisageable de localiser et d'extraire les données signifiantes du texte. Il apparaît alors que ces éléments signifiants sont, d'une certaine manière, des formes, ou des motifs qui peuvent être détectés automatiquement; la signification peut être identifiée en fonction du type de donnée extraite (un nombre peut être interprété comme une date). Mais, dans un corpus semi-structuré, il est surtout possible d'inférer du \enquote{sens} d'un élément à partir de sa position dans le texte. C'est dans ce cadre qu'un encodage en \xmltei{} des documents prend tout son intérêt: les différentes parties d'un texte sont balisées sémantiquement, ce qui est essentiel au traitement automatisé du texte: il devient possible de traiter celui-ci non pas dans son intégralité, mais au niveau d'une unité sémantique très précise, comme le nom donné à un manuscrit (le \tname{}) ou une brève description (le \ttrait{}). La nature semi-structurée d'un corpus permet donc de faire un aller et retour entre la précision de l'encodage et la part d'irrégularité qui est inhérente aux données textuelles, ce pour développer des méthodes d'enrichissement sémantique et d'extraction d'informations basées sur la connaissance et l'analyse du corpus.

Au delà de l'étude des possibilités offertes par la nature semi-structurée d'un corpus, la question de la manière dont celui-ci peut être diffusé et réutilisé a sous-tendu l'ensemble de ce travail de recherche. Les outils développés ont étés rajoutés à la chaîne de traitement en faisant attention à ce qu'ils ne risquent pas de rendre inefficaces d'autres parties de cette chaîne. Celle-ci a été dans son ensemble pensée pour être réutilisable par le projet lorsque de nouvelles données seraient rajoutées. Pour les étapes d'enrichissement de données à l'aide de \wkd{}, par exemple, des \glspl{log} ont été mis au point pour éviter d'avoir à traiter les entrées qui ont déjà été manipulées préalablement. Cela permet de raccourcir fortement le temps d'exécution de cette étape, et facilite donc l'intégration continue de nouvelles données dans une chaîne de traitement qui ne cesse de se complexifier. Mais les outils développés ne s'inscrivent pas vraiment dans une optique d'ouverture et de réutilisation de la recherche si ils sont uniquement voués à être utilisés en interne. C'est pourquoi les pages d'imprimés ont été segmentées à l'aide de \textit{SegmOnto}, une ontologie développée pour permettre d'uniformiser les pratiques de segmentation dans un contexte d'humanités numériques. Cela permet d'envisager le partage des données transcrites et encodées pour produire de la vérité de terrain pour les imprimés du \scl{XIX}. Il serait, dans cette optique, intéressant d'intégrer les catalogues transcrits à \textit{HTR United}, une plateforme pour le partage de vérité de terrain\footcite{chague_sharing_2022}. Mais les données qui seraient partagées sur cette plateforme sont en fait produites avant le début de la chaîne de traitement de \mssktb{}; si les seules données réutilisables sont partagées avant même d'avoir été enrichies avec tous les outils produits par le projet, alors le volet \enquote{science ouverte} de celui-ci reste relativement limité. C'est pourquoi l'application \textit{KatAPI} a été développée. Celle-ci vise justement à partager des données après qu'elles aient traversé l'intégralité de la chaîne de traitement. Dans le développement de cet outil, la complexité inhérente aux données textuelles issues des humanités apparaît encore. En effet, suivre des standards issus de l'informatique, comme le \gls{rest}, ne suffit pas pour partager des données de qualité. Pour rendre la recherche en sciences humaines réutilisable, un véritable travail d'édition des (méta-)données a été nécessaire. L'éditorialisation des réponses envoyées par l'\api{} correspond, tout comme l'encodage des catalogues, à un processus de modélisation. Comme l'encodage des catalogues encore, cette modélisation s'appuie d'abord sur une connaissance du corpus et sur l'établissement d'un schéma abstrait dans lequel intégrer les données. Ce travail d'édition vise à diffuser des réponses qui soient documentées et auto-descriptives, et donc facilement réutilisables sans avoir besoin de se référer à des sources extérieures. Dans le design d'\gls{api} pour la recherche, le respect de standards de qualité scientifique, comme le \gls{fair}, est au moins aussi important que respect de standards en développement Web.

Le fait de travailler sur des corpus semi-structurés permet également de mettre en œuvre des méthodes alternatives, \enquote{low-tech}, pour le traitement du langage et l'analyse textuelle dans des projets de recherche. Avec une bonne connaissance du corpus et des données structurées, il est possible de se baser entièrement sur des solutions techniquement \enquote{simples}, telles que la détection de motifs à l'aide d'\glspl{expression régulière}. Plutôt que de s'appuyer sur de l'apprentissage machine, et donc des solutions complexes, il est intéressant de s'appuyer sur des méthodes alternatives et de développer des algorithmes plus complexes, basés sur une approche modulaire, qui sont capables de cibler très précisément les éléments d'un texte pertinent pour des problématiques de recherche précises. Ces méthodes de basse technologie, qui s'intéressent à un usage intelligent des technologies plutôt qu'à des technologies \enquote{intelligentes} forment un contre modèle qui gagnerait à être mis en avant dans les projets de recherche. L'intelligence artificielle devenue un outil central et pertinent dans les humanités numériques -- à la base de la reconnaissance optique de caractères. Mais elle ne doit pas seulement être une nouveauté que les projets de recherche doivent à tout prix incorporer. C'est également une technologie extrêmement polluante\footnote{
	Strubell, Ganesh et McCallum (\cite{strubell_energy_2019}) ont étudié l'impact écologique des modèles d'apprentissage profond en traitement du langage naturel. En étudiant les ressources nécessaires à l'entraînement d'un modèle, les auteur.ice.s montrent (\cite[p. 1]{strubell_energy_2019}) que la mise au point d'un modèle \textit{Transformer} peut émettre jusqu'à 626155 livres de CO\textsubscript{2}, soit cinq fois la consommation d'une voiture durant toute sa vie (126000 livres de CO\textsubscript{2}) et 17 fois les émissions de CO\textsubscript{2} émises par une personne américaine en un an (36156 livres). Il.elle.s rappellent également que les modèles d'apprentissage machine les plus performants sont aussi les plus gourmands en ressources et qu'un modèle doit être entraîné parfois plusieurs milliers de fois pour atteindre des scores satisfaisants (\cite[p. 1]{strubell_energy_2019}). Avec l'augmentation générale des performances des modèles de traitement du langage naturel, l'augmentation marginale de leur performance est de plus en plus coûteuse en ressources: l'augmentation du \gls{bleu} d'un modèle de traduction automatique de 0.1 points de performance (pour atteindre un score de 29,7) a coûté 150000 dollars en ressources environnementales et en coût de calcul (\cite[p. 4]{strubell_energy_2019}).
}, utilisée de façon massive par des grands groupes industriels\footnote{
	\textit{Google}, et sa filiale \textit{Youtube}, s'appuient depuis le milieu des années 2010 de plus en plus massivement sur l'intelligence artificielle (\cite[p. 1]{covington_deep_2016}). L'algorithme de suggestion de vidéos sur \textit{Youtube} repose entièrement sur de l'apprentissage machine(\textit{ibid.}): en fonction de l'historique de l'utilisateur.ice, l'algorithme \enquote{apprend} à suggérer du contenu pertinent. L'usage massif de telles technologies est particulièrement polluant: pour pouvoir suggérer des vidéos pertinentes, il faut, à chaque connexion de l'utilisateur.ice, consulter deux très grandes bases de données: dans un premier temps, l'algorithme sélectionne toutes les vidéos potentiellement pertinentes, depuis l'ensemble des vidéos disponibles sur le site; ensuite, ces vidéos sont classées en consultant une seconde base de données et en filtrant les résultats obtenus par le comportement récent de l'utilisateur.ice (ce qu'il ou elle a recherché, combien de temps il ou elle a passé à regarder une vidéo, sa réaction à celle-ci...). Le 2 décembre 2020, \textit{Google} a \href{https://googlewalkout.medium.com/setting-the-record-straight-isupporttimnit-believeblackwomen-5d7bbfe4ed90}{licencié la docteure Timnit Gebru}, alors à la tête de l'équipe spécialisée en éthique de l'intelligence artificielle, suite à la rédaction d'un article sur les impacts écologiques de l'utilisation massive par l'entreprise de modèles de traitement automatisé du langage. Ce phénomène alarmant rappelle au passage la non-neutralité des publications scientifiques provenant ce cette entreprise, dont deux ont été citées dans le présent mémoire (\cite{mikolov_efficient_2013, covington_deep_2016}).
} et qui sert de plus en plus souvent d'argument de vente, alors que les applications pratiques de l'intelligence artificielle soulèvent de graves problèmes éthiques\footnote{
	À ce sujet, voir par exemple le projet \href{https://github.com/daviddao/awful-ai}{\textit{Awful AI} de David Dao}, qui liste une quantité impressionnante d'utilisations préoccupantes de telles technologies.
}. La recherche en humanités numériques, fondée sur une connaissance des techniques et des documents utilisés, devrait fournir un espace pour remettre en question de telles technologies et pour développer des solutions alternatives, où la connaissance scientifique supplante l'intelligence machine. De façon modeste, le projet \mssktb{} offre une telle remise en question: en s'appuyant sur une bonne connaissance de corpus textuels semi-structurés, le projet a développé une chaîne de traitement entièrement basée sur la détection de motifs. S'appuyer sur des méthodes de basse technologie ne vaut bien sûr pas seulement comme une déclaration de bonne foi: plus une solution est techniquement simple et accessible, plus elle est réutilisable, adaptable à d'autres jeux de données et plus facilement elle peut évoluer pour incorporer de nouvelles fonctionnalités. 

Si de telles approches de basse technologie sont sûrement développées par de nombreux projet en dehors de \mssktb{}, il semble cependant que les publications de recherche en humanités numériques mettent régulièrement en avant les approches basées sur l'apprentissage machine. Cela peut s'expliquer de plusieurs manières. D'abord, il existe aujourd'hui un attrait et une fascination globales pour l'apprentissage machine; ensuite, ces technologies sont très visibles dans la recherche en humanités numériques car elles sont très coûteuses, et leur usage doit donc être légitimé par des publications; enfin, ces technologies ont l'avantage d'être potentiellement adaptables à n'importe quelle source de données, alors que les méthodes de basse technologies présentées ici sont par définition spécifiques à un corpus. Pour rendre la recherche en humanités plus durable, et permettre le développement de méthodes alternatives, il serait cependant important d'augmenter la quantité de publications faites sur l'usage de méthodes de basse technologie pour la fouille de texte et l'exploitation de données. Il serait enfin important de trouver un moyen de rendre ces méthodes de basse technologie plus faciles à réutiliser et à adapter à d'autres corpus de texte, afin de permettre une convergence de méthodes et l'émergence de pratiques approuvées par une plus grande communauté de chercheur.euse.s.
